{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#IMPORT PACKAGES\n",
    "from rdflib import Graph\n",
    "import rdflib\n",
    "import os.path\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import open3d as o3d\n",
    "import uuid    \n",
    "import pye57 \n",
    "import ifcopenshell\n",
    "import ifcopenshell.geom as geom\n",
    "import ifcopenshell.util\n",
    "from ifcopenshell.util.selector import Selector\n",
    "import multiprocessing\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "# from tabulate import tabulate\n",
    "import cv2\n",
    "import laspy\n",
    "import json\n",
    "from scipy.spatial.transform import Rotation   \n",
    "import copy\n",
    "import geomapi\n",
    "from geomapi.nodes import *\n",
    "import geomapi.utils as ut\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import geomapi.tools as tl\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "import matplotlib.pyplot as plt\n",
    "import geomapi.tools.progresstools as pt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='rooms'\n",
    "\n",
    "path=Path(os.getcwd()).parents[0]/'data'\n",
    "pcd_input_path=os.path.join(path,f'{name}_labels.laz')\n",
    "class_file=path/'_classes.json'\n",
    "\n",
    "# name=name.split('_')[0]\n",
    "# json_output_path=os.path.join(path,f'{name}_walls.json') \n",
    "# geometry_output_path= os.path.join(path,f'{name}_walls.obj') # these are the bounding surfaces of the reference levels (optional)\n",
    "output_dir = \"C:\\Data\\temp\"\n",
    "\n",
    "graphPath=str(path/f'{name}Graph.ttl')\n",
    "\n",
    "grid_resolution = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': [{'name': 'Unassigned', 'id': 255, 'temp_id': 0, 'color': '#9da2ab'}, {'name': 'Floors', 'id': 0, 'temp_id': 1, 'color': '#03c2fc'}, {'name': 'Ceilings', 'id': 1, 'temp_id': 2, 'color': '#e81416'}, {'name': 'Walls', 'id': 2, 'temp_id': 3, 'color': '#ffa500'}, {'name': 'Columns', 'id': 3, 'temp_id': 4, 'color': '#faeb36'}, {'name': 'Doors', 'id': 4, 'temp_id': 5, 'color': '#79c314'}, {'name': 'Windows', 'id': 5, 'temp_id': 6, 'color': '#4b369d'}], 'default': 255, 'type': 'semantic_segmentation', 'format': 'kitti', 'created_with': {'name': 'Saiga', 'version': '1.0.1'}}\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "with open(class_file, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Create a dictionary\n",
    "class_dict = {\n",
    "    'classes': json_data['classes'],\n",
    "    'default': json_data['default'],\n",
    "    'type': json_data['type'],\n",
    "    'format': json_data['format'],\n",
    "    'created_with': json_data['created_with']\n",
    "}\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 wallNodes detected!\n",
      "32 wallNodes detected!\n",
      "10 ceilingsNodes detected!\n",
      "11 floorsNodes detected!\n",
      "160 clutterNodes detected!\n"
     ]
    }
   ],
   "source": [
    "graph=Graph().parse(graphPath)\n",
    "nodes=tl.graph_to_nodes(graph)\n",
    "wallBIMNodes=[n for n in nodes if 'Walls' in n.subject and type(n)==BIMNode]\n",
    "wallPCDNodes=[n for n in nodes if 'Walls' in n.subject and type(n)==PointCloudNode]\n",
    "ceilingsNodes=[n for n in nodes if 'Ceilings' in n.subject and type(n)==PointCloudNode]\n",
    "floorsNodes=[n for n in nodes if 'Floors' in n.subject and type(n)==PointCloudNode]\n",
    "clutterNodes=[n for n in nodes if 'Clutter' in n.subject and type(n)==PointCloudNode]\n",
    "print(f'{len(wallBIMNodes)} wallNodes detected!')\n",
    "print(f'{len(wallPCDNodes)} wallNodes detected!')\n",
    "print(f'{len(ceilingsNodes)} ceilingsNodes detected!')\n",
    "print(f'{len(floorsNodes)} floorsNodes detected!')\n",
    "print(f'{len(clutterNodes)} clutterNodes detected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_ifcPath': None, '_globalId': None, '_cartesianBounds': array([-13.48931333,  -9.36400216, -17.70804689, -17.00778565,\n",
      "         2.43      ,   4.85      ]), '_orientedBounds': array([[-13.46424761, -17.70804689,   4.85      ],\n",
      "       [ -9.36400216, -17.21714738,   4.85      ],\n",
      "       [-13.46424761, -17.70804689,   2.43      ],\n",
      "       [-13.48931333, -17.49868516,   4.85      ],\n",
      "       [ -9.38906787, -17.00778565,   2.43      ],\n",
      "       [-13.48931333, -17.49868516,   2.43      ],\n",
      "       [ -9.38906787, -17.00778565,   4.85      ],\n",
      "       [ -9.36400216, -17.21714738,   2.43      ]]), '_orientedBoundingBox': None, '_subject': rdflib.term.URIRef('file:///2_Walls_182_BIM'), '_graph': <Graph identifier=N6a9c10a799db4bcdbb8eff3a5eb905f0 (<class 'rdflib.graph.Graph'>)>, '_graphPath': None, '_path': None, '_name': None, '_timestamp': None, '_resource': None, '_cartesianTransform': array([[  1.        ,   0.        ,   0.        , -11.42665774],\n",
      "       [  0.        ,   1.        ,   0.        , -17.35791627],\n",
      "       [  0.        ,   0.        ,   1.        ,   3.64      ],\n",
      "       [  0.        ,   0.        ,   0.        ,   1.        ]]), 'type': 'https://w3id.org/v4d/core#BIMNode', 'pointCount': 8, 'base_constraint': 'file:///level_00', 'base_constraint_name': 'level_00', 'base_offset': 0.02448448125656233, 'color': '[0.24705882 0.01568627 0.1372549 ]', 'derivedFrom': 'file:///2_Walls_182', 'endpoint': '[ -9.37653501 -17.11246652   2.43      ]', 'faceCount': 12, 'height': 2.4200000000000004, 'normal': '[-0.11887547  0.99290917  0.        ]', 'startpoint': '[-13.47678047 -17.60336603   2.43      ]', 'top_constraint': 'file:///level_00', 'top_constraint_name': 'level_00', 'top_offset': 2.4444844812565627, 'wallLength': 4.129527230163991, 'wallThickness': 0.21085688161842892}\n"
     ]
    }
   ],
   "source": [
    "print({key:value for key, value in wallBIMNodes[1].__dict__.items() if not key.startswith('__') and not callable(key)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import PCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "laz=laspy.read(pcd_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match point clouds with graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in clutterNodes:#+ceilingsNodes+floorsNodes: # this is quite slow because you iterate through 2 scalar fields every time\n",
    "    idx=np.where((laz['classes']==n.class_id) & (laz['objects']==n.object_id))\n",
    "    pcd=o3d.geometry.PointCloud()\n",
    "    pcd.points=o3d.utility.Vector3dVector(laz.xyz[idx])\n",
    "    red = laz['red'][idx]\n",
    "    green = laz['green'][idx]\n",
    "    blue = laz['blue'][idx]\n",
    "    #if color is 32 bit, only keep 8 bit color\n",
    "    if red.max()>255:\n",
    "        red = laz['red'][idx] >> 8 & 0xFF\n",
    "        green = laz['green'][idx] >> 8 & 0xFF\n",
    "        blue = laz['blue'][idx] >> 8 & 0xFF\n",
    "    # if colorspace is [0-255] -> remap to [0-1]\n",
    "    if red.max() >1:\n",
    "        red=red/255\n",
    "        green=green/255\n",
    "        blue=blue/255\n",
    "    pcd.colors=o3d.utility.Vector3dVector(np.vstack((red,green,blue)).transpose())\n",
    "\n",
    "    n.resource=pcd\n",
    "    n.get_oriented_bounding_box()\n",
    "    n.orientedBoundingBox.color=[1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in wallPCDNodes:#+ceilingsNodes+floorsNodes: # this is quite slow because you iterate through 2 scalar fields every time\n",
    "    idx=np.where((laz['classes']==n.class_id) & (laz['objects']==n.object_id))\n",
    "    pcd=o3d.geometry.PointCloud()\n",
    "    pcd.points=o3d.utility.Vector3dVector(laz.xyz[idx])\n",
    "    pcd.paint_uniform_color([0.5,0.5,0.5])\n",
    "    \n",
    "    red = laz['red'][idx]\n",
    "    green = laz['green'][idx]\n",
    "    blue = laz['blue'][idx]\n",
    "    #if color is 32 bit, only keep 8 bit color\n",
    "    if red.max()>255:\n",
    "        red = laz['red'][idx] >> 8 & 0xFF\n",
    "        green = laz['green'][idx] >> 8 & 0xFF\n",
    "        blue = laz['blue'][idx] >> 8 & 0xFF\n",
    "    # if colorspace is [0-255] -> remap to [0-1]\n",
    "    if red.max() >1:\n",
    "        red=red/255\n",
    "        green=green/255\n",
    "        blue=blue/255\n",
    "    \n",
    "    pcd.colors=o3d.utility.Vector3dVector(np.column_stack((red, green, blue)))\n",
    "\n",
    "    n.resource=pcd\n",
    "    n.get_oriented_bounding_box()\n",
    "    n.orientedBoundingBox.color=[1,0,0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match PointCloudNodes to BIMNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m wallBIMNodes:\n\u001b[0;32m----> 2\u001b[0m     n\u001b[38;5;241m.\u001b[39mderivedFrom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwallPCDNodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderivedFrom\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwallBIMNodes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(n\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in wallBIMNodes:\n",
    "    n.derivedFrom = next(p for p in wallPCDNodes if p.subject.toPython() in [w.derivedFrom for w in wallBIMNodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_pcd=gmu.join_geometries([n.resource.paint_uniform_color(ut.literal_to_array(n.color)) for n in wallPCDNodes if n.resource is not None])\n",
    "# o3d.visualization.draw_geometries([joined_pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Reference Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 levelNodes detected!\n"
     ]
    }
   ],
   "source": [
    "levelNodes=[n for n in nodes if 'level' in n.subject]\n",
    "referenceNodes=[]\n",
    "for l in levelNodes:\n",
    "    new_graph=ut.get_subject_graph(graph,levelNodes[0].subject)\n",
    "    n=SessionNode(graph=new_graph)\n",
    "    n.get_oriented_bounding_box()\n",
    "    n.resource=o3d.geometry.TriangleMesh.create_from_oriented_bounding_box(n.orientedBoundingBox)\n",
    "    referenceNodes.append(n) # something is wrong in the tl.graph_to_nodes function\n",
    "levelNodes=referenceNodes\n",
    "print(f'{len(levelNodes)} levelNodes detected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import ceilings and floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in ceilingsNodes+floorsNodes: # this is quite slow because you iterate through 2 scalar fields every time\n",
    "    idx=np.where((laz['classes']==n.class_id) & (laz['objects']==n.object_id))\n",
    "    pcd=o3d.geometry.PointCloud()\n",
    "    pcd.points=o3d.utility.Vector3dVector(laz.xyz[idx])\n",
    "    n.resource=pcd\n",
    "    n.get_oriented_bounding_box()\n",
    "    n.orientedBoundingBox.color=[1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dtrings from the graph to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m wallBIMNodes:\n\u001b[0;32m----> 2\u001b[0m     n\u001b[38;5;241m.\u001b[39mstartpoint \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m      3\u001b[0m     n\u001b[38;5;241m.\u001b[39mendpoint \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n\u001b[38;5;241m.\u001b[39mendpoint[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m      4\u001b[0m     n\u001b[38;5;241m.\u001b[39mnormal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n\u001b[38;5;241m.\u001b[39mnormal[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "for n in wallBIMNodes:\n",
    "    n.startpoint = np.asarray(n.startpoint[1:-1].split(), dtype=float)\n",
    "    n.endpoint = np.asarray(n.endpoint[1:-1].split(), dtype=float)\n",
    "    n.normal = np.asarray(n.normal[1:-1].split(), dtype=float)\n",
    "    n.height = float(n.height)\n",
    "    n.name = n.subject.split('///')[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_on_line(point1, point2, step_size):\n",
    "    \"\"\"\n",
    "    Generate points on a line between two given points with a specified step size.\n",
    "\n",
    "    Parameters:\n",
    "    - point1: The starting point of the line.\n",
    "    - point2: The ending point of the line.\n",
    "    - step_size: The step size between consecutive points.\n",
    "\n",
    "    Returns:\n",
    "    - points: A list of points on the line.\n",
    "    \"\"\"\n",
    "    # Calculate the direction vector\n",
    "    direction = point2 - point1\n",
    "\n",
    "    # Calculate the length of the line segment\n",
    "    length = np.linalg.norm(direction)\n",
    "\n",
    "    # Normalize the direction vector\n",
    "    direction /= length\n",
    "\n",
    "    # Calculate the number of steps needed\n",
    "    num_steps = int(length / step_size)\n",
    "\n",
    "    # Generate points along the line\n",
    "    points = np.array([point1 + i * step_size * direction for i in range(num_steps + 1)])\n",
    "\n",
    "    return points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the full resolution point cloud for a more accurate result (unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "laz = laspy.read(os.path.join(Path(os.getcwd()).parents[0]/'data',\"full_resolution_populierenhof.las\"))\n",
    "full_res_point_cloud_o3d = gmu.las_to_pcd(laz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the full point cloud into a mesh and add it to a raycasting scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Cut a part out of the full resolution pointcloud\n",
    "# joined_pcd = full_res_point_cloud_o3d.crop(expanded_bounding_box)\n",
    "#Create a messh from this point cloud \n",
    "octree=pt.pcd_to_octree(full_res_point_cloud_o3d,10) #if octree is None else octree\n",
    "mesh=gmu.octree_to_voxelmesh(octree) #if mesh is None else mesh\n",
    "\n",
    "\n",
    "#Create a identity array containing the color so this can be retrieved afterwards\n",
    "original_colors=np.asarray(mesh.vertex_colors)\n",
    "indices=np.asarray(mesh.triangles)[:,0]\n",
    "triangle_colors=original_colors[indices]\n",
    "#append black color at the end of the array for the invalid hits\n",
    "triangle_colors=np.vstack((triangle_colors,np.array([0,0,0])))\n",
    "\n",
    "# Create raycasting scene\n",
    "scene = o3d.t.geometry.RaycastingScene()\n",
    "mesh=o3d.t.geometry.TriangleMesh.from_legacy(mesh)\n",
    "scene.add_triangles(mesh) \n",
    "\n",
    "# Calculate the size of each octree node based on octree depth and overall size\n",
    "def calculate_node_size(octree_depth, octree_size):\n",
    "    num_voxels_per_dim = 2 ** octree_depth\n",
    "    voxel_size = octree_size / num_voxels_per_dim\n",
    "    return voxel_size\n",
    "\n",
    "# Example usage:\n",
    "octree_depth = octree.max_depth  # Example value for max_depth\n",
    "octree_size = octree.size  # Example size of the octree in world units\n",
    "voxel_size = calculate_node_size(octree_depth, octree_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using the pointcloud and wall data to retrieve potential openings in the walls \n",
    "(Can also be used to retrieve wall detailing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opening_pcds(startpoint, endpoint, height, resolution, direction, scene, offset =0.7, path = None, show = False, min_samples = 5000, eps = 0.2, wallThickness = 0.1, voxel_size = 0.01):\n",
    "    \n",
    "    min_z = np.min([endpoint[2], startpoint[2]])\n",
    "    max_z = min_z + height\n",
    "    num_z_steps = int(height /resolution)\n",
    "    z_grid = np.linspace(min_z, max_z, num_z_steps)  # Adjust the number of grid points as needed\n",
    "    xyz_grid = []\n",
    "    for z in z_grid:\n",
    "        start = n.startpoint.copy()\n",
    "        end =  n.endpoint.copy()\n",
    "        start[2] = z\n",
    "        end[2] = z\n",
    "        xyz_grid.append(points_on_line(start, end, resolution))\n",
    "\n",
    "    grid = np.asarray(xyz_grid).reshape((-1, 3), order='C') \n",
    "    \n",
    "    openingpcds = []\n",
    "    # Create Open3D point cloud\n",
    "    grid_center_pcd = o3d.geometry.PointCloud()\n",
    "    grid_center_pcd.points = o3d.utility.Vector3dVector(np.asarray(grid))\n",
    "    grid_center_pcd.paint_uniform_color([1,0,0])\n",
    "\n",
    "    #In face is the dominant side of the wall\n",
    "    grid_in = grid + direction*offset\n",
    "    #out face is the other side of the dominant side\n",
    "    grid_out = grid - direction*offset\n",
    "\n",
    "    #create rays for the in side (towards the dominant side\n",
    "    ori_x = direction[0] * np.ones(len(grid))\n",
    "    ori_y = direction[1] * np.ones(len(grid))\n",
    "    ori_z = direction[2] * np.ones(len(grid))\n",
    "    \n",
    "    pos_x = grid_in[:,0]\n",
    "    pos_y = grid_in[:,1]\n",
    "    pos_z = grid_in[:,2]\n",
    "\n",
    "    # Stack the calculated values along the third axis to create the grid\n",
    "    rays_in_values = np.stack((pos_x, pos_y, pos_z, -ori_x, -ori_y, -ori_z), axis=1)\n",
    "    rays_in_tensor = o3d.core.Tensor(rays_in_values, dtype=o3d.core.Dtype.Float32)\n",
    "\n",
    "    pos_x = grid_out[:,0]\n",
    "    pos_y = grid_out[:,1]\n",
    "    pos_z = grid_out[:,2]\n",
    "\n",
    "    rays_out_values = np.stack((pos_x, pos_y, pos_z, ori_x, ori_y, ori_z), axis=1)\n",
    "    rays_out_tensor = o3d.core.Tensor(rays_out_values, dtype=o3d.core.Dtype.Float32)       \n",
    "\n",
    "    ans_in = scene.cast_rays(rays_in_tensor)\n",
    "    ans_out = scene.cast_rays(rays_out_tensor)\n",
    "\n",
    "    hits_in = ans_in['t_hit'].numpy()\n",
    "    hits_out = ans_out['t_hit'].numpy()\n",
    "\n",
    "    opening_points = []\n",
    "    opening_colors = []\n",
    "    \n",
    "    colors = np.zeros((len(hits_in), 3))\n",
    "    for i, c in enumerate(colors):\n",
    "        if hits_in[i] > 2*offset and hits_out[i] > 2*offset:\n",
    "            colors[i] = [0,1,0]\n",
    "            opening_points.append(grid[i])\n",
    "            opening_colors.append([0,1,0])\n",
    "\n",
    "        elif hits_in[i] <2*offset or hits_out[i] < 2*offset:\n",
    "            colors[i] = [0.5,0.5,0.5]\n",
    "            thickness = 2*offset-hits_out[i]-hits_in[i]\n",
    "    \n",
    "            if 1.2*voxel_size < thickness < 0.5*wallThickness:\n",
    "                # colors[i] = [1,0.5,0]\n",
    "                opening_points.append(grid[i])\n",
    "                opening_colors.append([0,1,0])\n",
    "\n",
    "\n",
    "    # grid_center_pcd.colors =o3d.utility.Vector3dVector(np.asarray(colors))\n",
    "    opening_pcd = o3d.geometry.PointCloud()\n",
    "    opening_pcd.points = o3d.utility.Vector3dVector(np.asarray(opening_points))\n",
    "    opening_pcd.colors = o3d.utility.Vector3dVector(np.asarray(opening_colors))\n",
    "    \n",
    "    opening_pcd, ind = opening_pcd.remove_radius_outlier(nb_points = 100, radius = 0.1)\n",
    "\n",
    "    # o3d.visualization.draw_geometries([opening_pcd])\n",
    "\n",
    "    points = np.asarray(opening_pcd.points)\n",
    "    if len(points) > min_samples:\n",
    "\n",
    "        # Perform clustering using DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(points)\n",
    "\n",
    "        # Extract unique cluster labels (excluding noise label -1)\n",
    "        unique_labels = np.unique(labels[labels != -1])\n",
    "\n",
    "        # Iterate over each cluster label and save corresponding points to a separate point cloud\n",
    "        for label in unique_labels:\n",
    "            cluster_points = points[labels == label]\n",
    "            cluster_pcd = o3d.geometry.PointCloud()\n",
    "            cluster_pcd.points = o3d.utility.Vector3dVector(cluster_points)\n",
    "            openingpcds.append(cluster_pcd)\n",
    "    if show:\n",
    "        o3d.visualization.draw_geometries(openingpcds)\n",
    "    # o3d.visualization.draw_geometries([grid_center_pcd])\n",
    "   \n",
    "    return openingpcds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_resolution = 0.01\n",
    "\n",
    "for n in wallBIMNodes:\n",
    "    \n",
    "    length = np.sqrt(np.sum((n.endpoint - n.startpoint)**2))\n",
    "    surface = length * n.height\n",
    "    n.openings1 = []\n",
    "\n",
    "    if not surface < 3 and n.height > 1.5 and length > 0.8:        \n",
    "        n.openings1 = create_opening_pcds(startpoint = n.startpoint, endpoint = n.endpoint, height=n.height, resolution= 0.01, direction= n.normal, scene= scene, offset =0.5, path = None, show = False, min_samples = 1000, eps = 0.25, wallThickness = n.wallThickness, voxel_size=voxel_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_door(openingwidth, openingheight, t_min_width = 0.5, t_max_width = 3, t_min_height = 1.5, t_max_height = 2.3):\n",
    "\n",
    "    if t_min_width < openingwidth < t_max_width and t_min_height < openingheight < t_max_height:\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Width: 1.67\n",
      "Opening height: 0.35260223048327166\n",
      "Opening Width: 1.6600000000000013\n",
      "Opening height: 0.3324626865671645\n",
      "Opening Width: 1.18\n",
      "Opening height: 0.8531954887218047\n",
      "Opening Width: 2.62\n",
      "Opening height: 0.5219548872180448\n",
      "Opening Width: 1.6400000000000003\n",
      "Opening height: 0.23086466165413544\n",
      "Opening Width: 3.350000000000001\n",
      "Opening height: 2.69\n",
      "Opening Width: 2.59\n",
      "Opening height: 1.7766044776119396\n",
      "This is most likely a door\n",
      "Opening Width: 2.67\n",
      "Opening height: 1.746492537313432\n",
      "This is most likely a door\n",
      "Opening Width: 5.090000000000001\n",
      "Opening height: 2.7199999999999998\n",
      "Opening Width: 2.8000000000000016\n",
      "Opening height: 2.3774814814814813\n",
      "Opening Width: 1.1499999999999992\n",
      "Opening height: 2.0581232209737825\n",
      "This is most likely a door\n",
      "Opening Width: 1.3800000000000003\n",
      "Opening height: 2.067715355805244\n",
      "This is most likely a door\n",
      "Opening Width: 0.4600000000000007\n",
      "Opening height: 0.7427715355805242\n",
      "Opening Width: 0.52\n",
      "Opening height: 0.632359550561798\n",
      "Opening Width: 0.9300000000000006\n",
      "Opening height: 2.0677443609022563\n",
      "This is most likely a door\n",
      "Opening Width: 0.9300000000000013\n",
      "Opening height: 2.0677443609022563\n",
      "This is most likely a door\n",
      "Opening Width: 0.9199999999999995\n",
      "Opening height: 2.0677443609022563\n",
      "This is most likely a door\n",
      "Opening Width: 1.2899999999999996\n",
      "Opening height: 1.1844360902255646\n",
      "Opening Width: 4.800000000000001\n",
      "Opening height: 0.26097744360902286\n",
      "Opening Width: 2.429999999999998\n",
      "Opening height: 0.2509398496240598\n",
      "Opening Width: 0.8900000000000001\n",
      "Opening height: 2.088630705394191\n",
      "This is most likely a door\n",
      "Opening Width: 0.7900000000000007\n",
      "Opening height: 1.265228215767635\n",
      "Opening Width: 2.509999999999999\n",
      "Opening height: 2.32974358974359\n",
      "Opening Width: 2.330000000000001\n",
      "Opening height: 2.3095726495726496\n",
      "Opening Width: 1.1600000000000006\n",
      "Opening height: 2.6999999999999997\n",
      "Opening Width: 0.4899999999999995\n",
      "Opening height: 0.46343283582089523\n",
      "Opening Width: 0.8399999999999992\n",
      "Opening height: 2.69\n",
      "Opening Width: 0.8899999999999986\n",
      "Opening height: 2.127910447761194\n",
      "This is most likely a door\n",
      "Opening Width: 0.9700000000000008\n",
      "Opening height: 2.117873134328358\n",
      "This is most likely a door\n",
      "Opening Width: 0.4299999999999984\n",
      "Opening height: 0.4617164179104476\n",
      "Opening Width: 0.9900000000000004\n",
      "Opening height: 2.0576779026217227\n",
      "This is most likely a door\n",
      "Opening Width: 0.41000000000000053\n",
      "Opening height: 0.3914606741573037\n",
      "Opening Width: 0.8300000000000005\n",
      "Opening height: 2.0576779026217227\n",
      "This is most likely a door\n"
     ]
    }
   ],
   "source": [
    "doors = []\n",
    "other = []\n",
    "for n in wallBIMNodes:\n",
    "    n.baseConstraint = next(l for l in levelNodes if l.subject.toPython() in [w.base_constraint for w in wallBIMNodes])\n",
    "    \n",
    "    if len(n.openings1) > 0:\n",
    "        for opening in n.openings1:\n",
    "            # Extract the z-coordinates\n",
    "            points2 = np.asarray(opening.points)\n",
    "            z_values = points2[:, 2]\n",
    "            unique_z_values = np.unique(z_values)\n",
    "\n",
    "            #Compute the width of the opening\n",
    "            opening_width = 0.0\n",
    "            for z_value in unique_z_values:\n",
    "                # Get points with the current z-value\n",
    "                points_with_same_z = points2[z_values == z_value]\n",
    "\n",
    "                # Find the outermost points based on XY coordinates\n",
    "                min_x = np.min(points_with_same_z[:, 0])\n",
    "                max_x = np.max(points_with_same_z[:, 0])\n",
    "                min_y = np.min(points_with_same_z[:, 1])\n",
    "                max_y = np.max(points_with_same_z[:, 1])\n",
    "\n",
    "                # Compute the diagonal length of the bounding box\n",
    "                diagonal_length = np.linalg.norm([max_x - min_x, max_y - min_y])\n",
    "\n",
    "                if diagonal_length > opening_width:\n",
    "                    opening_width = diagonal_length\n",
    "                    \n",
    "            print(\"Opening Width:\", opening_width)\n",
    "            \n",
    "            #Compute the Height of the door\n",
    "            lowest_z = np.min(z_values)\n",
    "            highest_z = np.max(z_values)\n",
    "            opening_height = highest_z - lowest_z   \n",
    "            print(\"Opening height:\", opening_height)\n",
    "            \n",
    "            if is_door(openingwidth = opening_width, openingheight = opening_height, t_min_width = 0.5, t_max_width = 3, t_min_height = 1.5, t_max_height = 2.3):\n",
    "                doors.append(opening)\n",
    "                print(\"This is most likely a door\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o3d.visualization.draw_geometries(doors)\n",
    "joined_pcd=gmu.join_geometries(doors)\n",
    "o3d.io.write_point_cloud(os.path.join(Path(os.getcwd()).parents[0]/'data', \"doors.pcd\"), joined_pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using the pointcloud and wall data to create an ortho foto of the wall and use object detection\n",
    "(Can also be used to retrieve other elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wall_ortho(startpoint, endpoint, height, resolution, direction, scene, offset =0.7, path = None, show = False, dominant = True):\n",
    "    image_size = (int(np.sqrt(np.sum((endpoint - startpoint)**2)) / resolution)+1, int(height / resolution))\n",
    "    \n",
    "    min_z = np.min([endpoint[2], startpoint[2]])\n",
    "    max_z = min_z + height\n",
    "    num_z_steps = int(height /resolution)\n",
    "    z_grid = np.linspace(min_z, max_z, num_z_steps)  # Adjust the number of grid points as needed\n",
    "    z_grid = z_grid[::-1]\n",
    "    xyz_grid = []\n",
    "    for z in z_grid:\n",
    "        start = startpoint.copy()\n",
    "        end =  endpoint.copy()\n",
    "        start[2] = z\n",
    "        end[2] = z\n",
    "        if not dominant:\n",
    "            xyz_grid.append(points_on_line(start, end, resolution)[::-1])\n",
    "        else: \n",
    "            xyz_grid.append(points_on_line(start, end, resolution))\n",
    "\n",
    "    grid = np.asarray(xyz_grid).reshape((-1, 3), order='C') \n",
    "    ray_grid = grid + direction*offset\n",
    "    \n",
    "    #create rays for the in side (towards the dominant side\n",
    "    ori_x = direction[0] * np.ones(len(ray_grid))\n",
    "    ori_y = direction[1] * np.ones(len(ray_grid))\n",
    "    ori_z = direction[2] * np.ones(len(ray_grid))\n",
    "    \n",
    "    pos_x = ray_grid[:,0]\n",
    "    pos_y = ray_grid[:,1]\n",
    "    pos_z = ray_grid[:,2]\n",
    "    \n",
    "    # Stack the calculated values along the third axis to create the grid\n",
    "    rays_values = np.stack((pos_x, pos_y, pos_z, -ori_x, -ori_y, -ori_z), axis=1)\n",
    "    rays_tensor = o3d.core.Tensor(rays_values, dtype=o3d.core.Dtype.Float32)\n",
    "    \n",
    "    ans= scene.cast_rays(rays_tensor) \n",
    "    \n",
    "    triangle_ids = ans[\"primitive_ids\"].numpy() # triangles     \n",
    "    triangle_ids = triangle_ids.flatten()\n",
    "    np.put(triangle_ids,np.where(triangle_ids==scene.INVALID_ID),triangle_colors.shape[0]-1) # replace invalid id's by last (which is the above added black color)\n",
    "    \n",
    "    colors = triangle_colors[triangle_ids]\n",
    "    ortho = np.reshape(colors,(image_size[1],image_size[0],3))\n",
    "    \n",
    "    if show:\n",
    "        plt.imshow(ortho)\n",
    "        plt.show()\n",
    "    if path:\n",
    "        image = Image.fromarray((ortho * 255).astype(np.uint8))\n",
    "        # Save the image\n",
    "        image.save(path)\n",
    "        \n",
    "    return ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resolution = 0.01\n",
    "\n",
    "for n in wallBIMNodes:\n",
    "    \n",
    "    length = np.sqrt(np.sum((n.endpoint - n.startpoint)**2))\n",
    "    surface = length * n.height\n",
    "    image_size = (int(length / image_resolution), int(n.height / image_resolution))\n",
    "    n.orthos = []\n",
    "    \n",
    "\n",
    "    if not surface < 3 and n.height > 1.5 and length > 0.8:  \n",
    "        #Create an ortho of the dominant side of the wall      \n",
    "        ortho = create_wall_ortho(startpoint = n.startpoint, endpoint= n.endpoint, height= n.height, resolution = image_resolution, direction = n.normal, scene=scene)\n",
    "        n.orthos.append(ortho)\n",
    "        #Also create an ortho of the other side of the wall\n",
    "        ortho = create_wall_ortho(startpoint = n.startpoint, endpoint= n.endpoint, height= n.height, resolution = image_resolution, direction = -n.normal, scene=scene, dominant = False)\n",
    "        n.orthos.append(ortho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounding DINO\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict\n",
    "from groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "# from segment_anything import build_sam, SamPredictor \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# diffusers\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
    "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file) \n",
    "    model = build_model(args)\n",
    "    args.device = device\n",
    "\n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this command for evaluate the Grounding DINO model\n",
    "# Or you can download the model by yourself\n",
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Model loaded from /home/sdegeyter/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "SupervisionWarnings: BoxAnnotator is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m load_image(Image\u001b[38;5;241m.\u001b[39mfromarray((n\u001b[38;5;241m.\u001b[39morthos[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)))\n\u001b[1;32m      8\u001b[0m boxes, logits, phrases \u001b[38;5;241m=\u001b[39m predict(\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mgroundingdino_model, \n\u001b[1;32m     10\u001b[0m     image\u001b[38;5;241m=\u001b[39mimage, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     text_threshold\u001b[38;5;241m=\u001b[39mTEXT_TRESHOLD\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m \u001b[43mannotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morthos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphrases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphrases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m annotated_frame[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# BGR to RGB\u001b[39;00m\n\u001b[1;32m     19\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(annotated_frame)\n",
      "File \u001b[0;32m~/Code/Scan-to-BIM-CVPR-2024/thirdparty/groundingdino/groundingdino/util/inference.py:138\u001b[0m, in \u001b[0;36mannotate\u001b[0;34m(image_source, boxes, logits, phrases)\u001b[0m\n\u001b[1;32m    131\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphrase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogit\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m phrase, logit\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(phrases, logits)\n\u001b[1;32m    135\u001b[0m ]\n\u001b[1;32m    137\u001b[0m box_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mBoxAnnotator()\n\u001b[0;32m--> 138\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_RGB2BGR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m box_annotator\u001b[38;5;241m.\u001b[39mannotate(scene\u001b[38;5;241m=\u001b[39mannotated_frame, detections\u001b[38;5;241m=\u001b[39mdetections, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m annotated_frame\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    }
   ],
   "source": [
    "for n in wallBIMNodes[0:10]:\n",
    "    TEXT_PROMPT = \"Door . Window\"\n",
    "    BOX_TRESHOLD = 0.3\n",
    "    TEXT_TRESHOLD = 0.3\n",
    "\n",
    "    image = load_image(Image.fromarray((n.orthos[0] * 255).astype(np.uint8)))\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=groundingdino_model, \n",
    "        image=image, \n",
    "        caption=TEXT_PROMPT, \n",
    "        box_threshold=BOX_TRESHOLD, \n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "\n",
    "    annotated_frame = annotate(n.orthos[0], boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[...,::-1] # BGR to RGB\n",
    "\n",
    "    Image.fromarray(annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m BOX_TRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m      3\u001b[0m TEXT_TRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m----> 5\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mortho\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m boxes, logits, phrases \u001b[38;5;241m=\u001b[39m predict(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mgroundingdino_model, \n\u001b[1;32m      9\u001b[0m     image\u001b[38;5;241m=\u001b[39mimage, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     text_threshold\u001b[38;5;241m=\u001b[39mTEXT_TRESHOLD\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m annotate(image_source\u001b[38;5;241m=\u001b[39mimage_source, boxes\u001b[38;5;241m=\u001b[39mboxes, logits\u001b[38;5;241m=\u001b[39mlogits, phrases\u001b[38;5;241m=\u001b[39mphrases)\n",
      "File \u001b[0;32m~/Code/Scan-to-BIM-CVPR-2024/thirdparty/groundingdino/groundingdino/util/inference.py:62\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     53\u001b[0m transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m     54\u001b[0m     [\n\u001b[1;32m     55\u001b[0m         T\u001b[38;5;241m.\u001b[39mRandomResize([\u001b[38;5;241m800\u001b[39m], max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1333\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     ]\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# image_source = Image.open(image_path).convert(\"RGB\")\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# image = np.asarray(image_source)\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m image_transformed, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_transformed\n",
      "File \u001b[0;32m~/Code/Scan-to-BIM-CVPR-2024/thirdparty/groundingdino/groundingdino/datasets/transforms.py:302\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, image, target)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, target):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m--> 302\u001b[0m         image, target \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[0;32m~/Code/Scan-to-BIM-CVPR-2024/thirdparty/groundingdino/groundingdino/datasets/transforms.py:234\u001b[0m, in \u001b[0;36mRandomResize.__call__\u001b[0;34m(self, img, target)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    233\u001b[0m     size \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msizes)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/Scan-to-BIM-CVPR-2024/thirdparty/groundingdino/groundingdino/datasets/transforms.py:116\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, target, size, max_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m get_size_with_aspect_ratio(image_size, size, max_size)\n\u001b[0;32m--> 116\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mget_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m rescaled_image \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mresize(image, size)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/Scan-to-BIM-CVPR-2024/thirdparty/groundingdino/groundingdino/datasets/transforms.py:114\u001b[0m, in \u001b[0;36mresize.<locals>.get_size\u001b[0;34m(image_size, size, max_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m size[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_size_with_aspect_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/Scan-to-BIM-CVPR-2024/thirdparty/groundingdino/groundingdino/datasets/transforms.py:91\u001b[0m, in \u001b[0;36mresize.<locals>.get_size_with_aspect_ratio\u001b[0;34m(image_size, size, max_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_size_with_aspect_ratio\u001b[39m(image_size, size, max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 91\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m image_size\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m         min_original_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mmin\u001b[39m((w, h)))\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVPR2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
