{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T6. WALL RECONSTRUCTION\n",
    "\n",
    "In this script, we reconstruct parametric wall geometries from the instance segmentation and reference heights.\n",
    "Specifically, we need:\n",
    " - T2: instances of walls, ceilings and other objects\n",
    " - T5: reference levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#IMPORT PACKAGES\n",
    "from rdflib import Graph, URIRef\n",
    "import os.path\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import open3d as o3d\n",
    "import uuid    \n",
    "import pye57 \n",
    "import ifcopenshell\n",
    "import ifcopenshell.geom as geom\n",
    "import ifcopenshell.util\n",
    "from ifcopenshell.util.selector import Selector\n",
    "import multiprocessing\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "# from tabulate import tabulate\n",
    "import cv2\n",
    "import laspy\n",
    "import json\n",
    "from scipy.spatial.transform import Rotation   \n",
    "import copy\n",
    "import geomapi\n",
    "from geomapi.nodes import *\n",
    "import geomapi.utils as ut\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import geomapi.tools as tl\n",
    "\n",
    "import geomapi\n",
    "from geomapi.nodes import *\n",
    "import geomapi.utils as ut\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import geomapi.tools as tl\n",
    "import geomapi.tools.progresstools as pt\n",
    "\n",
    "#import utils\n",
    "import context \n",
    "import utils as utl\n",
    "import utils.t6_utils as t6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "path=Path(os.getcwd()).parents[2] # with MB this is 2\n",
    "\n",
    "input_folder_t4=path/'data'/'t4'/'test' \n",
    "# input_folder_t4=path/'data'/'t4'/'train' \n",
    "\n",
    "input_folder_t5=path/'data'/'t5'/'test' \n",
    "# input_folder_t5=path/'data'/'t5'/'train' \n",
    "\n",
    "\n",
    "class_file=path/'data'/'_classes.json'\n",
    "output_folder=path/'data'/'t6'/'test'\n",
    "# output_folder=path/'data'/'t6'/'train'\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#thresholds\n",
    "t_level=0.5 #max distance to reference levels to adopt their height\n",
    "t_inliers=0.3 # % inliers for a wall to consider its own members or the neirby ceilings and floors for its position\n",
    "t_distance=0.7 #max distance to find opposite wallfaces\n",
    "t_thickness=0.127 #max thickness of a wall\n",
    "t_trim=0.3 #max distance to trim a wall\n",
    "t_extend=0.7 #max distance to extend a wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': [{'name': 'unassigned', 'id': 255, 'temp_id': 0, 'color': '#9da2ab'}, {'name': 'floors', 'id': 0, 'temp_id': 1, 'color': '#03c2fc'}, {'name': 'ceilings', 'id': 1, 'temp_id': 2, 'color': '#e81416'}, {'name': 'walls', 'id': 2, 'temp_id': 3, 'color': '#ffa500'}, {'name': 'columns', 'id': 3, 'temp_id': 4, 'color': '#faeb36'}, {'name': 'doors', 'id': 4, 'temp_id': 5, 'color': '#79c314'}], 'default': 255, 'type': 'semantic_segmentation', 'format': 'kitti', 'created_with': {'name': 'Saiga', 'version': '1.0.1'}}\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "with open(class_file, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Create a dictionary\n",
    "class_dict = {\n",
    "    'classes': json_data['classes'],\n",
    "    'default': json_data['default'],\n",
    "    'type': json_data['type'],\n",
    "    'format': json_data['format'],\n",
    "    'created_with': json_data['created_with']\n",
    "}\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 08_ShortOffice_01_F1_small_pred...\n",
      "Function match_graph_with_las took 10.7699 seconds to execute.\n",
      "111 pcdNodes and 5 levels found\n",
      "Estimating base_constraint...\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.012137923546972337\n",
      "name: level_20, base_constraint: level_00, base_offset: 0.43297823999999996\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.012137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.007862076453027663\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.31008207645302766\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.012137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.007862076453027663\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.007862076453027663\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.007862076453027663\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.007862076453027663\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.002137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.007862076453027663\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.3278620764530277\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.004612076453027671\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.002137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.002137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.012137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.002137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.002137923546972337\n",
      "name: level_00, base_constraint: level_00, base_offset: -0.027021760000000006\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.002137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.007862076453027663\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.012137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.002137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.007862076453027663\n",
      "name: level_10, base_constraint: level_10, base_offset: -0.012137923546972337\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.09786207645302766\n",
      "name: level_00, base_constraint: level_00, base_offset: -0.007021760000000099\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.007862076453027663\n",
      "name: level_10, base_constraint: level_10, base_offset: 0.4078620764530277\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#estimate parameters\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEstimating base_constraint...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mt6\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_base_constraint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwallNodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlevelNodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthreshold_level_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_level\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#there is a weakness here that takes the full point cloud while this is affected by outliers\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEstimating top_constraint...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m t6\u001b[38;5;241m.\u001b[39mcompute_top_constraint(wallNodes,levelNodes,threshold_level_height\u001b[38;5;241m=\u001b[39mt_level)\n",
      "File \u001b[1;32mc:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\code\\Scan-to-BIM-CVPR-2024-MB\\scripts\\..\\utils\\t6_utils.py:35\u001b[0m, in \u001b[0;36mcompute_base_constraint\u001b[1;34m(wallNodes, levelNodes, threshold_level_height)\u001b[0m\n\u001b[0;32m     33\u001b[0m             n\u001b[38;5;241m.\u001b[39mbase_constraint\u001b[38;5;241m=\u001b[39mn\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m         n\u001b[38;5;241m.\u001b[39mbase_constraint\u001b[38;5;241m=\u001b[39mlevelNodes[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mminheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlevelNodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43mminheight\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;66;03m#this is a link!\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         n\u001b[38;5;241m.\u001b[39mbase_constraint\u001b[38;5;241m=\u001b[39mlevelNodes[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39mbase_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m n\u001b[38;5;241m.\u001b[39mbase_constraint\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\u0094523\\.conda\\envs\\pointcept\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1338\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmin\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\u0094523\\.conda\\envs\\pointcept\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\u0094523\\.conda\\envs\\pointcept\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(asarray(obj), method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "point_cloud_files=utl.get_list_of_files(input_folder_t4,'.laz')\n",
    "level_files=utl.get_list_of_files(input_folder_t5,'.ttl')\n",
    "\n",
    "\n",
    "for f_pcd,f_rdf in zip(point_cloud_files[0:1],level_files[0:1]): #only read the first one\n",
    "    \n",
    "    print(f'processing {ut.get_filename(f_pcd)}...') \n",
    "    las = laspy.read(f_pcd) #if 'las' not in globals() else las\n",
    "    pcd=gmu.las_to_pcd(las) #if 'pcd' not in globals() else pcd # this is the slowest step\n",
    "        \n",
    "    #read point cloud nodes\n",
    "    pcdNodes=utl.match_graph_with_las(f_pcd,las,pcd,class_dict,getResources=True,getNormals=True)\n",
    "    \n",
    "    #read levelNodes\n",
    "    levelNodes=tl.graph_path_to_nodes(f_rdf)\n",
    "    for n in levelNodes:\n",
    "        n.resource=o3d.geometry.TriangleMesh.create_from_oriented_bounding_box(o3d.geometry.OrientedBoundingBox.create_from_points(o3d.utility.Vector3dVector(n.orientedBounds)))\n",
    "\n",
    "    #get wallNodes\n",
    "    wallNodes=[n for n in pcdNodes if n.class_id==2]\n",
    "    \n",
    "    #get ceiling and floor nodes\n",
    "    referenceNodes=[n for n in pcdNodes if n.class_id in[0,1]]\n",
    "    print(f'{len(wallNodes)} pcdNodes and {len(levelNodes)} levels found')  \n",
    "    \n",
    "    #estimate parameters\n",
    "    print('Estimating base_constraint...')\n",
    "    t6.compute_base_constraint(wallNodes,levelNodes,threshold_level_height=t_level) #there is a weakness here that takes the full point cloud while this is affected by outliers\n",
    "    print('Estimating top_constraint...')\n",
    "    t6.compute_top_constraint(wallNodes,levelNodes,threshold_level_height=t_level)\n",
    "    print('Estimating wall_orientation...')\n",
    "    t6.compute_wall_orientation(wallNodes,referenceNodes,t_thickness=t_thickness,t_distance=t_distance,t_inliers=t_inliers) \n",
    "    print('Estimating wall_thickness...') \n",
    "    t6.compute_wall_thickness(wallNodes,t_thickness=t_thickness,t_distance=t_distance)\n",
    "    print('Estimating wall_axis...')\n",
    "    t6.compute_wall_axis(wallNodes)\n",
    "    print('Estimating wall_topology...') \n",
    "    t6.trim_and_extend_wall_nodes(wallNodes,t_trim=t_trim,t_extend=t_extend) \n",
    "    print('Estimating wall_geometry...')\n",
    "    t6.compute_wall_geometry(wallNodes)\n",
    "    \n",
    "    #write this information to the 3D detection json\n",
    "    json_data=t6.walls_to_json(wallNodes,f_pcd)\n",
    "    with open(os.path.join(output_folder,f'{ut.get_filename(f_pcd)}_walls.json'), 'w') as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "    print(\"JSON data written to file:\", os.path.join(output_folder,f'{ut.get_filename(f_pcd)}_walls.json') )\n",
    "    \n",
    "    \n",
    "    #write the walls to obj\n",
    "    utl.write_obj_with_submeshes(os.path.join(output_folder,f'{ut.get_filename(f_pcd)}_walls.obj') , [n.wall for n in wallNodes], [n.name for n in wallNodes])\n",
    "    \n",
    "    #create BIMNodes\n",
    "    wallNodesBIM=[]\n",
    "    for i,n in enumerate(wallNodes):\n",
    "        b=BIMNode(subject=n.subject,\n",
    "                  name=n.name,\n",
    "                  derivedFrom=pcdNodes[i].subject,\n",
    "                    resource=n.wall,\n",
    "                    object_id=n.object_id,\n",
    "                    class_id=n.class_id,\n",
    "                    class_name=n.class_name,\n",
    "                    base_constraint=n.base_constraint.subject,\n",
    "                    base_constraint_name=n.base_constraint.name,\n",
    "                    base_offset=np.round(n.base_offset,3),\n",
    "                    top_constraint=n.top_constraint.subject,\n",
    "                    top_constraint_name=n.top_constraint.name,\n",
    "                    top_offset=np.round(n.top_offset,3),\n",
    "                    height=np.round(n.height,3), #gt\n",
    "                    width=np.round(n.wallThickness,3), #gt\n",
    "                    wallLength=np.round(n.wallLength,3), #gt\n",
    "                    normal=np.round(n.normal,3),\n",
    "                    axis=n.axis,\n",
    "                    start_pt=np.round(np.asarray(n.axis.points)[0],3), #gt\n",
    "                    end_pt=np.round(np.asarray(n.axis.points)[1],3), #gt\n",
    "                    neighbor_wall_ids_at_start=n.neighbor_wall_ids_at_start,\n",
    "                    neighbor_wall_ids_at_end=n.neighbor_wall_ids_at_end,\n",
    "                    color=n.color)\n",
    "        wallNodesBIM.append(b)        \n",
    "    graphPath=    os.path.join(output_folder,f'{ut.get_filename(f_pcd)}_walls.ttl')\n",
    "    new_graph=tl.nodes_to_graph(wallNodesBIM,graphPath=graphPath,save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_pcd=gmu.join_geometries([n.resource.paint_uniform_color(ut.literal_to_array(n.color)) for n in wallNodes if n.resource is not None])\n",
    "# o3d.visualization.draw_geometries([joined_pcd]+[n.resource for n in levelNodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3d.visualization.draw_geometries([joined_pcd]+\n",
    "#                                   [n.orientedBoundingBox for n in wallNodes]+\n",
    "#                                   [n.axis for n in wallNodes]+\n",
    "#                                   [o3d.geometry.PointCloud(o3d.utility.Vector3dVector(n.boundaryPoints)) for n in wallNodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_pcd=gmu.join_geometries([n.resource.paint_uniform_color(ut.literal_to_array(n.color)) for n in wallNodes if n.resource is not None])\n",
    "# sampled_ceilings_and_floors=gmu.sample_geometry(gmu.join_geometries([n.resource.paint_uniform_color(ut.literal_to_array(n.color)) for n in pcdNodes if n.class_id in[0,1]]))[0]\n",
    "for n in wallNodes:\n",
    "    if n.flipped:\n",
    "        n.wallBox.paint_uniform_color([1,0,0])\n",
    "o3d.visualization.draw_geometries([joined_pcd]+\n",
    "                                #   [sampled_ceilings_and_floors]+\n",
    "                                  [n.wallBox for n in wallNodes]+\n",
    "                                  [n.axis for n in wallNodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spawn potential connections (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential connections: 123 \n"
     ]
    }
   ],
   "source": [
    "# weight_intersection=4\n",
    "# weight_orthogonal=2\n",
    "# weight_direct=1\n",
    "\n",
    "# t_intersection_extension=2\n",
    "# t_direct_extension=0.5\n",
    "# t_ortho_extension=0.4\n",
    "\n",
    "# potential_connections=t6.compute_potential_wall_connections(wallNodesBIM,\n",
    "#                                       t_intersection_extension=t_intersection_extension,\n",
    "#                                       t_direct_extension=t_direct_extension,\n",
    "#                                       t_ortho_extension=t_ortho_extension,\n",
    "#                                       weight_intersection=weight_intersection,\n",
    "#                                       weight_orthogonal=weight_orthogonal,\n",
    "#                                       weight_direct=weight_direct)\n",
    "\n",
    "# print(f'potential connections: {np.sum([len(n.potential_connections) for n in wallNodesBIM]   )} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersections = gmu.join_geometries([l._resource.paint_uniform_color([1,0,0]) for n in wallNodesBIM for l in n.potential_connections if len(n.potential_connections) > 0 and l.weight == weight_intersection])\n",
    "# ortho= gmu.join_geometries([l._resource.paint_uniform_color([0,1,0]) for n in wallNodesBIM for l in n.potential_connections if len(n.potential_connections) > 0 and l.weight == weight_orthogonal])\n",
    "# direct= gmu.join_geometries([l._resource.paint_uniform_color([1,1,0]) for n in wallNodesBIM for l in n.potential_connections if len(n.potential_connections) > 0 and l.weight == weight_direct])\n",
    "# joined_pcd_base=gmu.join_geometries([n.new_axis.paint_uniform_color([0.3,0.3,0.3]) for n in wallNodesBIM if n.resource is not None])\n",
    "\n",
    "\n",
    "# o3d.visualization.draw_geometries([joined_pcd_base]+[intersections,ortho,direct])#[intersection1,intersection2]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wallNodesBIM[0].axis.paint_uniform_color([1,0,0])\n",
    "# intersection_pcd=o3d.geometry.PointCloud()\n",
    "# intersection_pcd.points=o3d.utility.Vector3dVector([item for sublist in [n.intersection_points for n in wallNodesBIM if getattr(n,'intersection_points',None) is not None] for item in sublist])\n",
    "# intersection_pcd.paint_uniform_color([1,0,0])\n",
    "# joined_lines1=gmu.join_geometries([n.axis for n in wallNodesBIM if n.resource is not None])\n",
    "# joined_lines2=gmu.join_geometries([n.new_axis for n in wallNodesBIM if n.resource is not None])\n",
    "\n",
    "\n",
    "# o3d.visualization.draw_geometries([intersection_pcd,joined_lines2])#[intersection1,intersection2]+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 pcdNodes_gt_walls detected!\n",
      "wall 35_Lab_02_F2_small1_walls_2000 with length 61.595, with a startpoint_diff of 34.63, endpoint_diff of 27.82, height_diff of 0.47, wallThickness_diff of -6.82 (gt width is 0.19), normal_diff of 0.9\n",
      "len of elements width_def <0.03: 0/1\n",
      "len of elements height_diff<0.1: 0/1\n",
      "len of elements normal_diff<0.05: 0/1\n",
      "len of elements axis location<0.1: 0/1\n",
      "len of computed elements with t_thickness: 0/1 vs gt elements 47/204\n",
      "len of elements with overlap_start>0.5: 0/1\n",
      "len of elements with overlap_start>0.5: 0/1\n",
      "in percentages\n",
      "len of elements width_def <0.03: 0.0%\n",
      "len of elements height_diff<0.1: 0.0%\n",
      "len of elements normal_diff<0.05: 0.0%\n",
      "len of elements axis location<0.1: 0.0%\n",
      "len of elements with overlap_start>0.5: 0.0%\n"
     ]
    }
   ],
   "source": [
    "#match the graphs with the training data\n",
    "\n",
    "input_folder_gt=path/'data'/'t1'/'train'\n",
    "\n",
    "gt_files=utl.get_list_of_files(input_folder_gt,'.ttl')\n",
    "gt_files_obj=utl.get_list_of_files(input_folder_gt,'.obj')\n",
    "for f_gt,f_gt_obj in zip(gt_files[6:7],gt_files_obj[6:7]): #only read the first one\n",
    "    \n",
    "    #import objects\n",
    "    mesh_dict=utl.load_obj_and_create_meshes(f_gt_obj)\n",
    "    \n",
    "    #import graph\n",
    "    pcdNodes_gt=tl.graph_path_to_nodes(graphPath=str(f_gt))\n",
    "    pcdNodes_gt_walls=[n for n in pcdNodes_gt if n.class_id==2]\n",
    "\n",
    "    #add objects to the nodes\n",
    "    for n in pcdNodes_gt_walls:\n",
    "        mesh=next((mesh for name, mesh in mesh_dict.items() if name == n.name),None)\n",
    "        n.resource=mesh\n",
    "        n.lineset=o3d.geometry.LineSet.create_from_triangle_mesh(mesh)\n",
    "        n.lineset.paint_uniform_color(ut.literal_to_array(n.color))\n",
    "        #compute the normal from start to end point\n",
    "        n.resource.compute_triangle_normals()\n",
    "        n.normal =np.asarray( n.resource.triangle_normals)[11]\n",
    "    print(f'{len(pcdNodes_gt_walls)} pcdNodes_gt_walls detected!') # there are only 144 walls in the training data, yet 161 are found here\n",
    "    \n",
    "    #match the walls with the ground truth\n",
    "    for n in wallNodesBIM:\n",
    "        \n",
    "        #create lineset\n",
    "        n.lineset=o3d.geometry.LineSet.create_from_triangle_mesh(n.resource)\n",
    "        n.lineset.paint_uniform_color(ut.literal_to_array(n.color))\n",
    "                \n",
    "        #find the corresponding ground truth wall \n",
    "        pose=n.cartesianTransform[:3,3]\n",
    "        distances=[np.linalg.norm(pose-n.cartesianTransform[:3,3]) for n in pcdNodes_gt_walls]\n",
    "        idx=np.argmin(distances)\n",
    "        n_gt=pcdNodes_gt_walls[idx]\n",
    "        n_gt.color=n.color\n",
    "        n.corresponding_gt=n_gt.name\n",
    "        n.corresponding_normal=n_gt.normal\n",
    "        n.corresponding_neighbor_wall_ids_at_start_gt=ut.literal_to_list(n_gt.neighbor_wall_ids_at_start)\n",
    "        n.corresponding_neighbor_wall_ids_at_start_gt=[int(x) for x in n.corresponding_neighbor_wall_ids_at_start_gt if x!=None] if n.corresponding_neighbor_wall_ids_at_start_gt is not None else [0]\n",
    "        n.corresponding_neighbor_wall_ids_at_end_gt=ut.literal_to_list(n_gt.neighbor_wall_ids_at_end)\n",
    "        n.corresponding_neighbor_wall_ids_at_end_gt=[int(x) for x in n.corresponding_neighbor_wall_ids_at_end_gt if x!=None] if n.corresponding_neighbor_wall_ids_at_end_gt is not None else [0]\n",
    "\n",
    "        #create a line from the n.resource.get_center() along the n.normal in red and along the n_gt.normal in green\n",
    "        # line=o3d.geometry.LineSet()\n",
    "        # line.points=o3d.utility.Vector3dVector([n.resource.get_center(),n.resource.get_center()+n.normal])\n",
    "        # line.lines=o3d.utility.Vector2iVector([[0,1]])\n",
    "        # line.colors=o3d.utility.Vector3dVector([[1,0,0]])\n",
    "        # n.lineset+=line\n",
    "        # line=o3d.geometry.LineSet()\n",
    "        # line.points=o3d.utility.Vector3dVector([n.resource.get_center(),n.resource.get_center()+n_gt.normal])\n",
    "        # line.lines=o3d.utility.Vector2iVector([[0,1]])\n",
    "        # line.colors=o3d.utility.Vector3dVector([[0,1,0]])\n",
    "        # n.lineset+=line\n",
    "        \n",
    "        \n",
    "        #make comparison\n",
    "        n.startpoint_diff=np.round(np.min(np.array([np.linalg.norm(ut.literal_to_array(n_gt.start_pt)[:2]-n.start_pt[:2]),np.linalg.norm(ut.literal_to_array(n_gt.end_pt)[:2]-n.start_pt[:2])])),2)\n",
    "        n.endpoint_diff=np.round(np.min(np.array([np.linalg.norm(ut.literal_to_array(n_gt.end_pt)[:2]-n.end_pt[:2]),np.linalg.norm(ut.literal_to_array(n_gt.start_pt)[:2]-n.end_pt[:2])])),2)\n",
    "        n.height_diff=np.round(n_gt.height-n.height,2) #CVPR height is arbitrary and not based on geometries\n",
    "        n.width_diff=np.round(n_gt.width-n.width,2)\n",
    "        n.normal_diff=np.round(1-np.abs(np.dot(n_gt.normal,n.normal)),2)\n",
    "        print(f'wall {n.get_name()} with length {n.wallLength}, with a startpoint_diff of {n.startpoint_diff}, endpoint_diff of {n.endpoint_diff}, height_diff of {n.height_diff}, wallThickness_diff of {n.width_diff} (gt width is {np.round(n_gt.width,2)}), normal_diff of {n.normal_diff}')\n",
    "\n",
    "    for n in wallNodesBIM:\n",
    "        \n",
    "        #find corresponding neighbors\n",
    "        n.corresponding_neighbor_wall_ids_at_start=[]\n",
    "        n.corresponding_neighbor_wall_ids_at_end=[]\n",
    "        for id in n.neighbor_wall_ids_at_start:\n",
    "            neighbor=next((m for m in wallNodesBIM if m.object_id==id),None)\n",
    "            node=next((m for m in pcdNodes_gt_walls if m.name==neighbor.corresponding_gt),None)\n",
    "            n.corresponding_neighbor_wall_ids_at_start.append(node.object_id)\n",
    "        for id in n.neighbor_wall_ids_at_end:\n",
    "            neighbor=next((m for m in wallNodesBIM if m.object_id==id),None)\n",
    "            node=next((m for m in pcdNodes_gt_walls if m.name==neighbor.corresponding_gt),None)\n",
    "            n.corresponding_neighbor_wall_ids_at_end.append(node.object_id)\n",
    "        #compute overlap between n.corresponding_neighbor_wall_ids_at_start_gt and n.corresponding_neighbor_wall_ids_at_start\n",
    "        n.overlap_start=len(set(n.corresponding_neighbor_wall_ids_at_start_gt).intersection(n.corresponding_neighbor_wall_ids_at_start))/len(n.corresponding_neighbor_wall_ids_at_start_gt)\n",
    "        n.overlap_end=len(set(n.corresponding_neighbor_wall_ids_at_end_gt).intersection(n.corresponding_neighbor_wall_ids_at_end))/len(n.corresponding_neighbor_wall_ids_at_end_gt)\n",
    "        # print(f'wall {n.get_name()} with overlap_start of {n.corresponding_neighbor_wall_ids_at_start} and gt {n.corresponding_neighbor_wall_ids_at_start_gt}')\n",
    "        \n",
    "    #general comparison    \n",
    "    print(f'len of elements width_def <0.03: {len([e for e in wallNodesBIM if np.abs(e.width_diff)<=0.03])}/{len(wallNodesBIM)}')\n",
    "    print(f'len of elements height_diff<0.1: {len([e for e in wallNodesBIM if np.abs(e.height_diff)<=0.1])}/{len(wallNodesBIM)}')\n",
    "    print(f'len of elements normal_diff<0.05: {len([e for e in wallNodesBIM if np.abs(e.normal_diff)<=0.05])}/{len(wallNodesBIM)}')    \n",
    "    print(f'len of elements axis location<0.1: {len([e for e in wallNodesBIM if (np.average(np.abs([e.startpoint_diff,e.endpoint_diff]))<=0.1)])}/{len(wallNodesBIM)}')  \n",
    "    print(f'len of computed elements with t_thickness: {len([e for e in wallNodesBIM if e.width ==t_thickness])}/{len(wallNodesBIM)} vs gt elements {len([e for e in pcdNodes_gt_walls if e.width == t_thickness])}/{len(pcdNodes_gt_walls)}')\n",
    "    print(f'len of elements with overlap_start>0.5: {len([e for e in wallNodesBIM if e.overlap_start>=0.5])}/{len(wallNodesBIM)}')\n",
    "    print(f'len of elements with overlap_start>0.5: {len([e for e in wallNodesBIM if e.overlap_end>=0.5])}/{len(wallNodesBIM)}')\n",
    "    \n",
    "    \n",
    "    #general comparison  \n",
    "    print('in percentages')  \n",
    "    print(f'len of elements width_def <0.03: {np.round(100*len([e for e in wallNodesBIM if np.abs(e.width_diff)<=0.03])/len(wallNodesBIM),1)}%')\n",
    "    print(f'len of elements height_diff<0.1: {np.round(100*len([e for e in wallNodesBIM if np.abs(e.height_diff)<=0.1])/len(wallNodesBIM),1)}%')\n",
    "    print(f'len of elements normal_diff<0.05: {np.round(100*len([e for e in wallNodesBIM if np.abs(e.normal_diff)<=0.05])/len(wallNodesBIM),1)}%')    \n",
    "    print(f'len of elements axis location<0.1: {np.round(100*len([e for e in wallNodesBIM if (np.average(np.abs([e.startpoint_diff,e.endpoint_diff]))<=0.1)])/len(wallNodesBIM),1)}%')  \n",
    "    print(f'len of elements with overlap_start>0.5: {np.round(100*np.average((len([e for e in wallNodesBIM if e.overlap_start>=0.5]),len([e for e in wallNodesBIM if e.overlap_end>=0.5])))/len(wallNodesBIM),1)}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_pcd=gmu.join_geometries([n.resource.paint_uniform_color(ut.literal_to_array(n.color)) for n in wallNodes if n.resource is not None])\n",
    "\n",
    "# # joined_pcd=gmu.join_geometries([n.lineset.paint_uniform_color(ut.literal_to_array(n.color)) for n in wallNodesBIM if n.resource is not None])\n",
    "# joined_pcd_bim=gmu.join_geometries([n.lineset.paint_uniform_color([1,0,0]) for n in wallNodesBIM if np.abs(n.width_diff) >0.03])\n",
    "\n",
    "# # joined_pcd_gt=gmu.join_geometries([n.lineset.paint_uniform_color(ut.literal_to_array(n.color)) for n in pcdNodes_gt_walls if n.resource is not None])\n",
    "# joined_pcd_gt=gmu.join_geometries([n.lineset.paint_uniform_color([0.3,0.3,0.3]) for n in pcdNodes_gt_walls if n.resource is not None])\n",
    "\n",
    "# o3d.visualization.draw_geometries([joined_pcd_bim,joined_pcd]+[joined_pcd_gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3d.visualization.draw_geometries([joined_pcd]+\n",
    "#                                   [n.wallBox for n in wallNodes]+\n",
    "#                                   [n.axis for n in wallNodes]+\n",
    "#                                   [o3d.geometry.PointCloud(o3d.utility.Vector3dVector(n.boundaryPoints)) for n in wallNodes])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomapi_installed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
