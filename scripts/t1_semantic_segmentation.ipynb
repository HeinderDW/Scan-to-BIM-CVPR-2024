{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1 SEMANTIC SEGMENTATION\n",
    "\n",
    "Import and prepare point clouds for semantic segmentation and do the inference.\n",
    "To run these scripts, create a python 3.10 environment & install geomapi (numpy, opend3d, ifcopenshell, trimesh, ...), pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT PACKAGES\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('.')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('scripts')))\n",
    "import numpy as np\n",
    "import laspy\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT DATA CONVERSION\n",
    "\n",
    "Preprocessing of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_path = Path(os.getcwd())/'data'/'t1_data'/'input'/'Eva.las'\n",
    "output_folder =  Path(os.getcwd())/'data'/'t1_data'/'input'\n",
    "\n",
    "scene_id = os.path.basename(point_cloud_path)\n",
    "\n",
    "name, ext = os.path.splitext(scene_id)\n",
    "\n",
    "if ext not in  [\".las\", \".laz\"]:\n",
    "    exit()\n",
    "\n",
    "# Read LAS/LAZ\n",
    "# populate dict\n",
    "las = laspy.read(point_cloud_path)\n",
    "print(list(las.point_format.dimension_names))\n",
    "\n",
    "pcd = gmu.las_to_pcd(las)\n",
    "pcd.estimate_normals()\n",
    "pcd.orient_normals_to_align_with_direction()\n",
    "\n",
    "coords = np.stack([las.x, las.y, las.z], axis=1)\n",
    "# colors = np.stack([las.red, las.green, las.blue], axis=1) # betons\n",
    "colors = np.zeros((len(las.points), 3)).astype(np.uint8) + 255\n",
    "normals = np.asarray(pcd.normals)    \n",
    "\n",
    "save_dict = dict(coord=coords, color=colors, normal=normals, scene_id=scene_id) #, semantic_gt=las.labels.astype(int))\n",
    "\n",
    "torch.save(save_dict, os.path.join(output_folder, f\"{name}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE\n",
    "\n",
    "Inference using Point Transformer V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointcept.engines.defaults import (\n",
    "    default_argument_parser,\n",
    "    default_config_parser,\n",
    "    default_setup,\n",
    ")\n",
    "from pointcept.engines.test import TESTERS\n",
    "from pointcept.engines.launch import launch\n",
    "from pointcept.datasets import build_dataset, collate_fn\n",
    "import pointcept.utils.comm as comm\n",
    "import torch\n",
    "import os\n",
    "from pointcept.engines.defaults import create_ddp_model\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import numpy as np\n",
    "from pointcept.utils.misc import make_dirs\n",
    "import torch.nn.functional as F\n",
    "from pointcept.models import build_model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "def build_inference_model(cfg):\n",
    "    model = build_model(cfg.model)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model = create_ddp_model(\n",
    "        model.cuda(),\n",
    "        broadcast_buffers=False,\n",
    "        find_unused_parameters=cfg.find_unused_parameters,\n",
    "    )\n",
    "    if os.path.isfile(cfg.weight):\n",
    "        checkpoint = torch.load(cfg.weight)\n",
    "        weight = OrderedDict()\n",
    "        for key, value in checkpoint[\"state_dict\"].items():\n",
    "            if key.startswith(\"module.\"):\n",
    "                if comm.get_world_size() == 1:\n",
    "                    key = key[7:]  # module.xxx.xxx -> xxx.xxx\n",
    "            else:\n",
    "                if comm.get_world_size() > 1:\n",
    "                    key = \"module.\" + key  # xxx.xxx -> module.xxx.xxx\n",
    "            weight[key] = value\n",
    "        model.load_state_dict(weight, strict=True)\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"=> No checkpoint found at '{}'\".format(cfg.weight))\n",
    "    return model\n",
    "\n",
    "def main_worker(cfg):    \n",
    "    cfg = default_setup(cfg)\n",
    "    test_dataset = build_dataset(cfg.data.test)\n",
    "\n",
    "    if comm.get_world_size() > 1:\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\n",
    "    else:\n",
    "        test_sampler = None\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg.batch_size_test_per_gpu,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.batch_size_test_per_gpu,\n",
    "        pin_memory=True,\n",
    "        sampler=test_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model = build_inference_model(cfg)\n",
    "    model.eval()\n",
    "\n",
    "    save_path = os.path.join(cfg.save_path, \"result\")\n",
    "    make_dirs(save_path)\n",
    "    \n",
    "    for idx, data_dict in enumerate(test_loader):\n",
    "        data_dict = data_dict[0]  # current assume batch size is 1\n",
    "        fragment_list = data_dict.pop(\"fragment_list\")\n",
    "        segment = data_dict.pop(\"segment\")\n",
    "        data_name = data_dict.pop(\"name\")\n",
    "        pred_save_path = os.path.join(save_path, \"{}_pred.npy\".format(data_name))\n",
    "\n",
    "        pred = torch.zeros((segment.size, cfg.data.num_classes)).cuda()\n",
    "        for i in range(len(fragment_list)):\n",
    "            fragment_batch_size = 1\n",
    "            s_i, e_i = i * fragment_batch_size, min(\n",
    "                (i + 1) * fragment_batch_size, len(fragment_list)\n",
    "            )\n",
    "            input_dict = collate_fn(fragment_list[s_i:e_i])[0]\n",
    "            for key in input_dict.keys():\n",
    "                if isinstance(input_dict[key], torch.Tensor):\n",
    "                    input_dict[key] = input_dict[key].cuda(non_blocking=True)\n",
    "            idx_part = input_dict[\"index\"]            \n",
    "            with torch.no_grad():\n",
    "                pred_part = model(input_dict)[\"seg_logits\"]  # (n, k)\n",
    "                pred_part = F.softmax(pred_part, -1)\n",
    "                if cfg.empty_cache:\n",
    "                    torch.cuda.empty_cache()\n",
    "                bs = 0                \n",
    "                for be in input_dict[\"offset\"]:\n",
    "                    pred[idx_part[bs:be], :] += pred_part[bs:be]\n",
    "                    bs = be        \n",
    "        pred = pred.max(1)[1].data.cpu().numpy()\n",
    "        np.save(pred_save_path, pred)\n",
    "\n",
    "    print(\"DONE.\")\n",
    "\n",
    "config_path = Path(os.getcwd())/'data'/'t1_data'/'config.py' #Path(os.getcwd())/'data'/'t1_data'/'config.py'\n",
    "save_path = Path(os.getcwd())/'data'/'t1_data' #Path(os.getcwd())/'data'/'t1_data'\n",
    "weights = Path(os.getcwd())/'data'/'t1_data'/'model'/'model_best.pth' #Path(os.getcwd())/'data'/'t1_data'/'model'/'model_best.pth'\n",
    "\n",
    "cfg = default_config_parser(str(config_path), {'save_path': str(save_path), 'weight': str(weights)})\n",
    "\n",
    "launch(\n",
    "    main_worker,\n",
    "    num_gpus_per_machine=1,\n",
    "    num_machines=1,\n",
    "    machine_rank=0,\n",
    "    dist_url='auto',\n",
    "    cfg=(cfg,),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS\n",
    "\n",
    "Convert inference result back to .las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "point_cloud_path = Path(os.getcwd())/'data'/'t1_data'/'input'/'Eva.las'\n",
    "labels_path = Path(os.getcwd())/'data'/'t1_data'/'result'/'Eva_pred.npy'\n",
    "output_folder =  Path(os.getcwd())/'data'/'t1_data'/'result'\n",
    "\n",
    "las = laspy.read(point_cloud_path)\n",
    "\n",
    "las.add_extra_dim(laspy.ExtraBytesParams(\n",
    "    name=\"labels\",\n",
    "    type=np.int32\n",
    "))\n",
    "\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "las[\"labels\"] = labels\n",
    "\n",
    "las.write(os.path.join(output_folder, \"Eva_pred.las\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 64-bit",
   "name": "python3919jvsc74a57bd0335c87f273ac958436761b9f67f775e8d80f72098ef3f48ec79b69099f6adb85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "metadata": {
   "interpreter": {
    "hash": "335c87f273ac958436761b9f67f775e8d80f72098ef3f48ec79b69099f6adb85"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
