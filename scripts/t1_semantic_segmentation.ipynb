{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1 SEMANTIC SEGMENTATION\n",
    "\n",
    "Import and prepare point clouds for semantic segmentation and do the inference.\n",
    "To run these scripts, create a python 3.10 environment & install geomapi (numpy, opend3d, ifcopenshell, trimesh, ...), pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET UP CORRECT FILE PATHS\n",
    "\n",
    "Please set and check files path based on your data folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_folder = os.path.abspath('..')\n",
    "print(root_folder)\n",
    "data_folder_name = os.path.join(root_folder, 'test_data', 't1_data_fbk')\n",
    "print(data_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT PACKAGES\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, root_folder)\n",
    "sys.path.insert(0, os.path.join(root_folder, \"scripts\"))\n",
    "sys.path.insert(0, os.path.join(root_folder, 'thirdparty', 'pointcept'))\n",
    "print(sys.path)\n",
    "import numpy as np\n",
    "import laspy\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT DATA CONVERSION\n",
    "\n",
    "Preprocessing of input data. Convert las/laz files to PyTorch (.pth) files in order to be processed by Pointcept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_process(file_name, output_folder):\n",
    "    \n",
    "    print(file_name)\n",
    "\n",
    "    scene_id = os.path.basename(file_name)\n",
    "\n",
    "    name, ext = os.path.splitext(scene_id)\n",
    "    \n",
    "    if ext not in  [\".las\", \".laz\"]:\n",
    "        return\n",
    "\n",
    "    # Read LAS/LAZ\n",
    "    # populate dict\n",
    "    las = laspy.read(file_name)\n",
    "    print(list(las.point_format.dimension_names))\n",
    "\n",
    "    pcd = gmu.las_to_pcd(las)\n",
    "    pcd.estimate_normals()\n",
    "    pcd.orient_normals_to_align_with_direction()\n",
    "    \n",
    "    coords = np.stack([las.x, las.y, las.z], axis=1)\n",
    "    colors = np.stack([las.red / 256, las.green / 256, las.blue / 256], axis=1).astype(np.uint8)\n",
    "    normals = np.asarray(pcd.normals)\n",
    "    \n",
    "    save_dict = dict(coord=coords, color=colors, normal=normals, scene_id=scene_id)\n",
    "\n",
    "    torch.save(save_dict, os.path.join(output_folder, f\"{name}.pth\"))\n",
    "\n",
    "inference_las_folder = os.path.join(data_folder_name, 'input', 'inference')\n",
    "inference_output_folder = os.path.join(data_folder_name, 'inference')\n",
    "\n",
    "os.makedirs(inference_output_folder, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(inference_las_folder):\n",
    "    handle_process(os.path.join(inference_las_folder, file_name), inference_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE\n",
    "\n",
    "Inference using Pointcept (Point Transformer V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointcept.engines.defaults import (\n",
    "    default_argument_parser,\n",
    "    default_config_parser,\n",
    "    default_setup,\n",
    ")\n",
    "from pointcept.engines.test import TESTERS\n",
    "from pointcept.engines.launch import launch\n",
    "from pointcept.datasets import build_dataset, collate_fn\n",
    "import pointcept.utils.comm as comm\n",
    "import torch\n",
    "import os\n",
    "from pointcept.engines.defaults import create_ddp_model\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import numpy as np\n",
    "from pointcept.utils.misc import make_dirs\n",
    "import torch.nn.functional as F\n",
    "from pointcept.models import build_model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "def build_inference_model(cfg):\n",
    "    model = build_model(cfg.model)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model = create_ddp_model(\n",
    "        model.cuda(),\n",
    "        broadcast_buffers=False,\n",
    "        find_unused_parameters=cfg.find_unused_parameters,\n",
    "    )\n",
    "    if os.path.isfile(cfg.weight):\n",
    "        checkpoint = torch.load(cfg.weight)\n",
    "        weight = OrderedDict()\n",
    "        for key, value in checkpoint[\"state_dict\"].items():\n",
    "            if key.startswith(\"module.\"):\n",
    "                if comm.get_world_size() == 1:\n",
    "                    key = key[7:]  # module.xxx.xxx -> xxx.xxx\n",
    "            else:\n",
    "                if comm.get_world_size() > 1:\n",
    "                    key = \"module.\" + key  # xxx.xxx -> module.xxx.xxx\n",
    "            weight[key] = value\n",
    "        model.load_state_dict(weight, strict=True)\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"=> No checkpoint found at '{}'\".format(cfg.weight))\n",
    "    return model\n",
    "\n",
    "def do_inference(cfg):    \n",
    "    cfg = default_setup(cfg)\n",
    "    test_dataset = build_dataset(cfg.data.test)\n",
    "    \n",
    "    if comm.get_world_size() > 1:\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\n",
    "    else:\n",
    "        test_sampler = None\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg.batch_size_test_per_gpu,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.batch_size_test_per_gpu,\n",
    "        pin_memory=True,\n",
    "        sampler=test_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model = build_inference_model(cfg)\n",
    "    model.eval()\n",
    "\n",
    "    save_path = os.path.join(cfg.save_path, \"result\")\n",
    "    make_dirs(save_path)\n",
    "    \n",
    "    for idx, data_dict in enumerate(test_loader):\n",
    "        data_dict = data_dict[0]  # current assume batch size is 1\n",
    "        fragment_list = data_dict.pop(\"fragment_list\")\n",
    "        segment = data_dict.pop(\"segment\")\n",
    "        data_name = data_dict.pop(\"name\")\n",
    "        pred_save_path = os.path.join(save_path, \"{}_pred.npy\".format(data_name))\n",
    "\n",
    "        pred = torch.zeros((segment.size, cfg.data.num_classes)).cuda()\n",
    "        for i in range(len(fragment_list)):\n",
    "            fragment_batch_size = 1\n",
    "            s_i, e_i = i * fragment_batch_size, min(\n",
    "                (i + 1) * fragment_batch_size, len(fragment_list)\n",
    "            )\n",
    "            input_dict = fragment_list[s_i:e_i][0] #collate_fn(fragment_list[s_i:e_i])[0]\n",
    "            for key in input_dict.keys():\n",
    "                if isinstance(input_dict[key], torch.Tensor):\n",
    "                    input_dict[key] = input_dict[key].cuda(non_blocking=True)\n",
    "            idx_part = input_dict[\"index\"]            \n",
    "            with torch.no_grad():\n",
    "                pred_part = model(input_dict)[\"seg_logits\"]  # (n, k)\n",
    "                pred_part = F.softmax(pred_part, -1)\n",
    "                if cfg.empty_cache:\n",
    "                    torch.cuda.empty_cache()\n",
    "                bs = 0                \n",
    "                for be in input_dict[\"offset\"]:\n",
    "                    pred[idx_part[bs:be], :] += pred_part[bs:be]\n",
    "                    bs = be        \n",
    "        pred = pred.max(1)[1].data.cpu().numpy()\n",
    "        np.save(pred_save_path, pred)\n",
    "\n",
    "    print(\"DONE.\")\n",
    "\n",
    "config_path = os.path.join(data_folder_name, 'config.py')\n",
    "save_path = data_folder_name\n",
    "weights = os.path.join(data_folder_name, 'model', 'model_best.pth')\n",
    "\n",
    "cfg = default_config_parser(str(config_path), {'save_path': str(save_path), 'weight': str(weights), 'data_root': str(save_path)})\n",
    "\n",
    "do_inference(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS\n",
    "\n",
    "Convert inference result back to laz and remap the classes to the correct ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "classes_file = os.path.join(data_folder_name, '..', '_classes.json')\n",
    "\n",
    "# Read the JSON file\n",
    "with open(classes_file, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "classes_list = json_data['classes']\n",
    "\n",
    "print(classes_list)\n",
    "\n",
    "remapped_classes_ids = {}\n",
    "\n",
    "for class_entry in classes_list:\n",
    "    remapped_classes_ids[int(class_entry[\"temp_id\"])] = int(class_entry[\"id\"])\n",
    "\n",
    "print(remapped_classes_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_result(las_file_path, predictions_folder, output_folder):\n",
    "    \n",
    "    las = laspy.read(las_file_path)\n",
    "\n",
    "    dimensions = list(las.header.point_format.dimension_names)\n",
    "\n",
    "    if not \"classes\" in dimensions:\n",
    "\n",
    "        las.add_extra_dim(laspy.ExtraBytesParams(\n",
    "            name=\"classes\",\n",
    "            type=np.int32\n",
    "        ))\n",
    "\n",
    "    file_name, _ = os.path.splitext(os.path.basename(las_file_path))\n",
    "\n",
    "    prediction_file_path = os.path.join(predictions_folder, f\"{file_name}_pred.npy\")\n",
    "\n",
    "    if not os.path.exists(prediction_file_path):\n",
    "        return\n",
    "\n",
    "    labels = np.load(prediction_file_path)\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    for label in labels:\n",
    "        classes.append(remapped_classes_ids[label])\n",
    "\n",
    "    las[\"classes\"] = np.array(classes).astype(np.uint8)\n",
    "\n",
    "    las.write(os.path.join(output_folder, f\"{file_name}_pred.laz\"))\n",
    "\n",
    "\n",
    "inference_las_folder = os.path.join(data_folder_name, 'input', 'inference')\n",
    "predictions_folder = os.path.join(data_folder_name, 'result')\n",
    "output_folder =  os.path.join(data_folder_name, 'result')\n",
    "\n",
    "for file_name in os.listdir(inference_las_folder):\n",
    "    process_result(os.path.join(inference_las_folder, file_name), predictions_folder, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "metadata": {
   "interpreter": {
    "hash": "335c87f273ac958436761b9f67f775e8d80f72098ef3f48ec79b69099f6adb85"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
