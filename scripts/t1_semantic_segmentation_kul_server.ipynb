{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1 SEMANTIC SEGMENTATION\n",
    "\n",
    "Import and prepare point clouds for semantic segmentation and do the inference.\n",
    "To run these scripts, create a python 3.10 environment & install geomapi (numpy, opend3d, ifcopenshell, trimesh, ...), pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#IMPORT PACKAGES\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import laspy\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import torch\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#CONTEXT\n",
    "import context \n",
    "\n",
    "#UTILS\n",
    "from utils.t1_utils import *\n",
    "\n",
    "#POINTCEPT\n",
    "from pointcept.engines.defaults import (\n",
    "    default_argument_parser,\n",
    "    default_config_parser,\n",
    "    default_setup,\n",
    ")\n",
    "from pointcept.engines.test import TESTERS\n",
    "from pointcept.engines.launch import launch\n",
    "from pointcept.datasets import build_dataset, collate_fn\n",
    "import pointcept.utils.comm as comm\n",
    "\n",
    "from pointcept.engines.defaults import create_ddp_model\n",
    "\n",
    "from pointcept.utils.misc import make_dirs\n",
    "from pointcept.models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mbassier/code/Scan-to-BIM-CVPR-2024/data/t1_data_test\n"
     ]
    }
   ],
   "source": [
    "path= Path(os.getcwd()).parents[0]/'data'/'t1_data_test'\n",
    "print( path)\n",
    "point_cloud_path =path/'input'/'Eva.las'\n",
    "input_folder =  path/'input'\n",
    "\n",
    "#TRAINING\n",
    "\n",
    "\n",
    "#INFERENCE\n",
    "config_path = path/'config.py' \n",
    "weights = path/'model'/'model_best.pth' \n",
    "\n",
    "\n",
    "#RESULTS\n",
    "labels_path = path/'result'/'Eva_pred.npy' # -> pred_save_path\n",
    "save_path =  path/'result'\n",
    "os.makedirs(save_path) if not save_path.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse config file\n",
    "cfg = default_config_parser(str(config_path), \n",
    "                            {'save_path': str(path), \n",
    "                             'weight': str(weights)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or directly run python from main repo\n",
    "export PYTHONPATH=\"/home/mbassier/code/Scan-to-BIM-CVPR-2024/thirdparty:$PYTHONPATH\" # run this if submodule is not found\n",
    "python thirdparty/pointcept/pointcept/tools/inference_kul.py --config-file /home/mbassier/code/Scan-to-BIM-CVPR-2024/thirdparty/pointcept/configs/kul/kul-pt-v3-base.py --options save_path=/home/mbassier/code/Scan-to-BIM-CVPR-2024/data/t1_data_test/result weight=/home/mbassier/code/Scan-to-BIM-CVPR-2024/data/t1_data_test/model/model_best.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT DATA CONVERSION\n",
    "\n",
    "Preprocessing of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'Z', 'intensity', 'return_number', 'number_of_returns', 'scan_direction_flag', 'edge_of_flight_line', 'classification', 'synthetic', 'key_point', 'withheld', 'scan_angle_rank', 'user_data', 'point_source_id', 'red', 'green', 'blue']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scene_id = os.path.basename(point_cloud_path)\n",
    "\n",
    "name, ext = os.path.splitext(scene_id)\n",
    "\n",
    "if ext not in  [\".las\", \".laz\"]:\n",
    "    exit()\n",
    "\n",
    "# Read LAS/LAZ\n",
    "# populate dict\n",
    "las = laspy.read(point_cloud_path)\n",
    "print(list(las.point_format.dimension_names))\n",
    "\n",
    "pcd = gmu.las_to_pcd(las)\n",
    "pcd.estimate_normals()\n",
    "pcd.orient_normals_to_align_with_direction()\n",
    "\n",
    "coords = np.stack([las.x, las.y, las.z], axis=1)\n",
    "# colors = np.stack([las.red, las.green, las.blue], axis=1) # betons\n",
    "colors = np.zeros((len(las.points), 3)).astype(np.uint8) + 255\n",
    "normals = np.asarray(pcd.normals)    \n",
    "\n",
    "save_dict = dict(coord=coords, color=colors, normal=normals, scene_id=scene_id) #, semantic_gt=las.labels.astype(int))\n",
    "\n",
    "torch.save(save_dict, os.path.join(input_folder, f\"{name}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE\n",
    "\n",
    "Inference using Point Transformer V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-09 11:05:52,932 INFO kul.py line 58 735719] Totally 0 x 1 samples in input set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "def build_inference_model(cfg):\n",
    "    model = build_model(cfg.model)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model = create_ddp_model(\n",
    "        model.cuda(),\n",
    "        broadcast_buffers=False,\n",
    "        find_unused_parameters=cfg.find_unused_parameters,\n",
    "    )\n",
    "    if os.path.isfile(cfg.weight):\n",
    "        checkpoint = torch.load(cfg.weight)\n",
    "        weight = OrderedDict()\n",
    "        for key, value in checkpoint[\"state_dict\"].items():\n",
    "            if key.startswith(\"module.\"):\n",
    "                if comm.get_world_size() == 1:\n",
    "                    key = key[7:]  # module.xxx.xxx -> xxx.xxx\n",
    "            else:\n",
    "                if comm.get_world_size() > 1:\n",
    "                    key = \"module.\" + key  # xxx.xxx -> module.xxx.xxx\n",
    "            weight[key] = value\n",
    "        model.load_state_dict(weight, strict=True)\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"=> No checkpoint found at '{}'\".format(cfg.weight))\n",
    "    return model\n",
    "\n",
    "def main_worker(cfg):    \n",
    "    cfg = default_setup(cfg)\n",
    "    test_dataset = build_dataset(cfg.data.test)\n",
    "    {key:value for key, value in test_dataset.__dict__.items() if not key.startswith('__') and not callable(key)}              \n",
    "\n",
    "    if comm.get_world_size() > 1:\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\n",
    "    else:\n",
    "        test_sampler = None\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg.batch_size_test_per_gpu,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.batch_size_test_per_gpu,\n",
    "        pin_memory=True,\n",
    "        sampler=test_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model = build_inference_model(cfg)\n",
    "    model.eval()\n",
    "   \n",
    "    for idx, data_dict in enumerate(test_loader):\n",
    "        data_dict = data_dict[0]  # current assume batch size is 1\n",
    "        fragment_list = data_dict.pop(\"fragment_list\")\n",
    "        segment = data_dict.pop(\"segment\")\n",
    "        data_name = data_dict.pop(\"name\")\n",
    "        pred_save_path = os.path.join(save_path, \"{}_pred.npy\".format(data_name))\n",
    "\n",
    "        pred = torch.zeros((segment.size, cfg.data.num_classes)).cuda()\n",
    "        for i in range(len(fragment_list)):\n",
    "            fragment_batch_size = 1\n",
    "            s_i, e_i = i * fragment_batch_size, min(\n",
    "                (i + 1) * fragment_batch_size, len(fragment_list)\n",
    "            )\n",
    "            input_dict = collate_fn(fragment_list[s_i:e_i])[0]\n",
    "            for key in input_dict.keys():\n",
    "                if isinstance(input_dict[key], torch.Tensor):\n",
    "                    input_dict[key] = input_dict[key].cuda(non_blocking=True)\n",
    "            idx_part = input_dict[\"index\"]            \n",
    "            with torch.no_grad():\n",
    "                pred_part = model(input_dict)[\"seg_logits\"]  # (n, k)\n",
    "                pred_part = F.softmax(pred_part, -1)\n",
    "                if cfg.empty_cache:\n",
    "                    torch.cuda.empty_cache()\n",
    "                bs = 0                \n",
    "                for be in input_dict[\"offset\"]:\n",
    "                    pred[idx_part[bs:be], :] += pred_part[bs:be]\n",
    "                    bs = be        \n",
    "        pred = pred.max(1)[1].data.cpu().numpy()\n",
    "        np.save(pred_save_path, pred)\n",
    "\n",
    "    print(\"DONE.\")\n",
    "\n",
    "\n",
    "launch(\n",
    "    main_worker,\n",
    "    num_gpus_per_machine=1,\n",
    "    num_machines=1,\n",
    "    machine_rank=0,\n",
    "    dist_url='auto',\n",
    "    cfg=(cfg,),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS\n",
    "\n",
    "Convert inference result back to .las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mbassier/code/Scan-to-BIM-CVPR-2024/data/t1_data_test/result/Eva_pred.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m las \u001b[38;5;241m=\u001b[39m laspy\u001b[38;5;241m.\u001b[39mread(point_cloud_path)\n\u001b[1;32m      5\u001b[0m las\u001b[38;5;241m.\u001b[39madd_extra_dim(laspy\u001b[38;5;241m.\u001b[39mExtraBytesParams(\n\u001b[1;32m      6\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32\n\u001b[1;32m      8\u001b[0m ))\n\u001b[0;32m---> 10\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m las[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m     14\u001b[0m las\u001b[38;5;241m.\u001b[39mwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEva_pred.las\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mbassier/code/Scan-to-BIM-CVPR-2024/data/t1_data_test/result/Eva_pred.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# las = laspy.read(point_cloud_path)\n",
    "\n",
    "las.add_extra_dim(laspy.ExtraBytesParams(\n",
    "    name=\"labels\",\n",
    "    type=np.int32\n",
    "))\n",
    "\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "las[\"labels\"] = labels\n",
    "\n",
    "las.write(os.path.join(save_path, \"Eva_pred.las\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "metadata": {
   "interpreter": {
    "hash": "335c87f273ac958436761b9f67f775e8d80f72098ef3f48ec79b69099f6adb85"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
