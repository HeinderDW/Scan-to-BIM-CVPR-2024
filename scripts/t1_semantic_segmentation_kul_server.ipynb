{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1 SEMANTIC SEGMENTATION\n",
    "\n",
    "Import and prepare point clouds for semantic segmentation and do the inference.\n",
    "To run these scripts, create a python 3.10 environment & install geomapi (numpy, opend3d, ifcopenshell, trimesh, ...), pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT PACKAGES\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import laspy\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import torch\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "#CONTEXT\n",
    "import context \n",
    "\n",
    "#UTILS\n",
    "from utils.t1_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mbassier/code/Scan-to-BIM-CVPR-2024/data/t1_data_test\n"
     ]
    }
   ],
   "source": [
    "path= Path(os.getcwd()).parents[0]/'data'/'t1_data_test'\n",
    "print( path)\n",
    "point_cloud_path =path/'input'/'Eva.las'\n",
    "input_folder =  path/'input'\n",
    "\n",
    "#TRAINING\n",
    "\n",
    "\n",
    "#INFERENCE\n",
    "config_path = path/'config.py' \n",
    "weights = path/'model'/'model_best.pth' \n",
    "\n",
    "\n",
    "#RESULTS\n",
    "class_file=path.parents[0]/'_classes.json'\n",
    "labels_path = path/'result'/'Eva_pred.npy' # -> pred_save_path\n",
    "save_path =  path/'result'\n",
    "os.makedirs(save_path) if not save_path.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse config file\n",
    "cfg = default_config_parser(str(config_path), \n",
    "                            {'save_path': str(path), \n",
    "                             'weight': str(weights)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #or directly run python from main repo\n",
    "# export PYTHONPATH=\"/home/mbassier/code/Scan-to-BIM-CVPR-2024/thirdparty:$PYTHONPATH\" # run this if submodule is not found\n",
    "# python thirdparty/pointcept/pointcept/tools/inference_kul.py --config-file /home/mbassier/code/Scan-to-BIM-CVPR-2024/thirdparty/pointcept/configs/kul/kul-pt-v3-base.py --options save_path=/home/mbassier/code/Scan-to-BIM-CVPR-2024/data/t1_data_test/result weight=/home/mbassier/code/Scan-to-BIM-CVPR-2024/data/t1_data_test/model/model_best.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT DATA CONVERSION\n",
    "\n",
    "Preprocessing of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'Z', 'intensity', 'return_number', 'number_of_returns', 'scan_direction_flag', 'edge_of_flight_line', 'classification', 'synthetic', 'key_point', 'withheld', 'scan_angle_rank', 'user_data', 'point_source_id', 'red', 'green', 'blue']\n"
     ]
    }
   ],
   "source": [
    "scene_id = os.path.basename(point_cloud_path)\n",
    "name, ext = os.path.splitext(scene_id)\n",
    "\n",
    "#Read LAS/LAZ\n",
    "las = laspy.read(point_cloud_path)\n",
    "print(list(las.point_format.dimension_names))\n",
    "\n",
    "pcd = gmu.las_to_pcd(las)\n",
    "pcd.estimate_normals()\n",
    "pcd.orient_normals_to_align_with_direction()\n",
    "\n",
    "#populate dict\n",
    "coords = np.stack([las.x, las.y, las.z], axis=1)\n",
    "colors=np.asarray(pcd.colors)\n",
    "normals = np.asarray(pcd.normals)    \n",
    "save_dict = dict(coord=coords, color=colors, normal=normals, scene_id=scene_id) #, semantic_gt=las.labels.astype(int))\n",
    "\n",
    "#save in input folder\n",
    "torch.save(save_dict, os.path.join(input_folder, f\"{name}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE\n",
    "\n",
    "Inference using Point Transformer V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-09 13:52:01,715 INFO kul.py line 58 735719] Totally 1 x 1 samples in input set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sampler is None\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "#POINTCEPT\n",
    "from pointcept.engines.defaults import (\n",
    "    default_argument_parser,\n",
    "    default_config_parser,\n",
    "    default_setup,\n",
    ")\n",
    "from pointcept.engines.test import TESTERS\n",
    "from pointcept.engines.launch import launch\n",
    "from pointcept.datasets import build_dataset, collate_fn\n",
    "import pointcept.utils.comm as comm\n",
    "\n",
    "from pointcept.engines.defaults import create_ddp_model\n",
    "\n",
    "from pointcept.utils.misc import make_dirs\n",
    "from pointcept.models import build_model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "def build_inference_model(cfg):\n",
    "    model = build_model(cfg.model)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model = create_ddp_model(\n",
    "        model.cuda(),\n",
    "        broadcast_buffers=False,\n",
    "        find_unused_parameters=cfg.find_unused_parameters,\n",
    "    )\n",
    "    if os.path.isfile(cfg.weight):\n",
    "        checkpoint = torch.load(cfg.weight)\n",
    "        weight = OrderedDict()\n",
    "        for key, value in checkpoint[\"state_dict\"].items():\n",
    "            if key.startswith(\"module.\"):\n",
    "                if comm.get_world_size() == 1:\n",
    "                    key = key[7:]  # module.xxx.xxx -> xxx.xxx\n",
    "            else:\n",
    "                if comm.get_world_size() > 1:\n",
    "                    key = \"module.\" + key  # xxx.xxx -> module.xxx.xxx\n",
    "            weight[key] = value\n",
    "        model.load_state_dict(weight, strict=True)\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"=> No checkpoint found at '{}'\".format(cfg.weight))\n",
    "    return model\n",
    "\n",
    "def main_worker(cfg):    \n",
    "    cfg = default_setup(cfg)\n",
    "    test_dataset = build_dataset(cfg.data.test)\n",
    "    \n",
    "    if comm.get_world_size() > 1:\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\n",
    "    else:\n",
    "        test_sampler = None\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg.batch_size_test_per_gpu,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.batch_size_test_per_gpu,\n",
    "        pin_memory=True,\n",
    "        sampler=test_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model = build_inference_model(cfg)\n",
    "    model.eval()\n",
    "   \n",
    "    for idx, data_dict in enumerate(test_loader):\n",
    "        data_dict = data_dict[0]  # current assume batch size is 1\n",
    "        fragment_list = data_dict.pop(\"fragment_list\")\n",
    "        segment = data_dict.pop(\"segment\")\n",
    "        data_name = data_dict.pop(\"name\")\n",
    "        pred_save_path = os.path.join(save_path, \"{}_pred.npy\".format(data_name))\n",
    "\n",
    "        pred = torch.zeros((segment.size, cfg.data.num_classes)).cuda()\n",
    "        for i in range(len(fragment_list)):\n",
    "            fragment_batch_size = 1\n",
    "            s_i, e_i = i * fragment_batch_size, min(\n",
    "                (i + 1) * fragment_batch_size, len(fragment_list)\n",
    "            )\n",
    "            input_dict = collate_fn(fragment_list[s_i:e_i])[0]\n",
    "            for key in input_dict.keys():\n",
    "                if isinstance(input_dict[key], torch.Tensor):\n",
    "                    input_dict[key] = input_dict[key].cuda(non_blocking=True)\n",
    "            idx_part = input_dict[\"index\"]            \n",
    "            with torch.no_grad():\n",
    "                pred_part = model(input_dict)[\"seg_logits\"]  # (n, k)\n",
    "                pred_part = F.softmax(pred_part, -1)\n",
    "                if cfg.empty_cache:\n",
    "                    torch.cuda.empty_cache()\n",
    "                bs = 0                \n",
    "                for be in input_dict[\"offset\"]:\n",
    "                    pred[idx_part[bs:be], :] += pred_part[bs:be]\n",
    "                    bs = be        \n",
    "        pred = pred.max(1)[1].data.cpu().numpy()\n",
    "        np.save(pred_save_path, pred)\n",
    "\n",
    "    print(\"DONE.\")\n",
    "\n",
    "\n",
    "launch(\n",
    "    main_worker,\n",
    "    num_gpus_per_machine=1,\n",
    "    num_machines=1,\n",
    "    machine_rank=0,\n",
    "    dist_url='auto',\n",
    "    cfg=(cfg,),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': [{'name': 'Unassigned', 'id': 255, 'temp_id': 0, 'color': '#9da2ab'}, {'name': 'Floors', 'id': 0, 'temp_id': 1, 'color': '#03c2fc'}, {'name': 'Ceilings', 'id': 1, 'temp_id': 2, 'color': '#e81416'}, {'name': 'Walls', 'id': 2, 'temp_id': 3, 'color': '#ffa500'}, {'name': 'Columns', 'id': 3, 'temp_id': 4, 'color': '#faeb36'}, {'name': 'Doors', 'id': 4, 'temp_id': 5, 'color': '#79c314'}, {'name': 'Windows', 'id': 5, 'temp_id': 6, 'color': '#4b369d'}], 'default': 255, 'type': 'semantic_segmentation', 'format': 'kitti', 'created_with': {'name': 'Saiga', 'version': '1.0.1'}}\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "with open(class_file, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Create a dictionary\n",
    "class_dict = {\n",
    "    'classes': json_data['classes'],\n",
    "    'default': json_data['default'],\n",
    "    'type': json_data['type'],\n",
    "    'format': json_data['format'],\n",
    "    'created_with': json_data['created_with']\n",
    "}\n",
    "print(class_dict)\n",
    "\n",
    "# Creating a dictionary to map temp_id to id\n",
    "temp_id_to_id = {cls[\"temp_id\"]: cls[\"id\"] for cls in class_dict[\"classes\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert inference result back to .las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# las = laspy.read(point_cloud_path)\n",
    "\n",
    "# Remapping function using numpy's vectorize to map temp_ids to ids\n",
    "id_mapper = np.vectorize(temp_id_to_id.get)\n",
    "temp_ids =np.load(labels_path)\n",
    "labels = id_mapper(temp_ids)\n",
    "\n",
    "\n",
    "las.add_extra_dim(laspy.ExtraBytesParams(\n",
    "    name=\"labels\",\n",
    "    type=np.int32\n",
    "))\n",
    "\n",
    "las[\"labels\"] = labels\n",
    "\n",
    "las.write(os.path.join(save_path, \"Eva_pred.las\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "metadata": {
   "interpreter": {
    "hash": "335c87f273ac958436761b9f67f775e8d80f72098ef3f48ec79b69099f6adb85"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
