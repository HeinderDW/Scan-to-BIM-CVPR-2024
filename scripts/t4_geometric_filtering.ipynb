{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4 Combine Detection Results\n",
    "\n",
    "- import the t1 inferenced point clouds\n",
    "- import the t3 doors \n",
    "- export graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#IMPORT PACKAGES\n",
    "from rdflib import Graph, URIRef\n",
    "import os.path\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import open3d as o3d\n",
    "import uuid    \n",
    "import pye57 \n",
    "import ifcopenshell\n",
    "import ifcopenshell.geom as geom\n",
    "import ifcopenshell.util\n",
    "from ifcopenshell.util.selector import Selector\n",
    "import multiprocessing\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "# from tabulate import tabulate\n",
    "import cv2\n",
    "import laspy\n",
    "\n",
    "import geomapi\n",
    "from geomapi.nodes import *\n",
    "import geomapi.utils as ut\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import geomapi.tools as tl\n",
    "import geomapi.tools.progresstools as pt\n",
    "\n",
    "#import utils\n",
    "import context \n",
    "import utils as utl\n",
    "import utils.t1_utils as t1\n",
    "import utils.t2_utils as t2\n",
    "import utils.t4_utils as t4\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\n"
     ]
    }
   ],
   "source": [
    "#paths\n",
    "path=Path(os.getcwd()).parents[2]\n",
    "\n",
    "print(path)\n",
    "input_folder=path/'data'/'t2'/'test' \n",
    "class_file=path/'data'/'_classes.json'\n",
    "output_folder=path/'data'/'t4'/ 'test'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#parameters\n",
    "resolution=0.05\n",
    "min_cluster_points=1000\n",
    "eps=0.5\n",
    "\n",
    "#doors\n",
    "threshold_door_dim=1.6#m\n",
    "\n",
    "#walls\n",
    "threshold_clustering_distance=0.4\n",
    "threshold_min_cluster_points=500\n",
    "threshold_wall_verticality=0.2# angle to horizontal\n",
    "threshold_wall_dim=0.5\n",
    "size=[12,12,100] #size wall boxes\n",
    "\n",
    "#columns\n",
    "threshold_column_points=1000\n",
    "threshold_column_height=1.5#m\n",
    "threshold_column_verticality=0.2# angle to horizontal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': [{'name': 'Unassigned', 'id': 255, 'temp_id': 0, 'color': '#9da2ab'}, {'name': 'floors', 'id': 0, 'temp_id': 1, 'color': '#03c2fc'}, {'name': 'ceilings', 'id': 1, 'temp_id': 2, 'color': '#e81416'}, {'name': 'walls', 'id': 2, 'temp_id': 3, 'color': '#ffa500'}, {'name': 'columns', 'id': 3, 'temp_id': 4, 'color': '#faeb36'}, {'name': 'doors', 'id': 4, 'temp_id': 5, 'color': '#79c314'}], 'default': 255, 'type': 'semantic_segmentation', 'format': 'kitti', 'created_with': {'name': 'Saiga', 'version': '1.0.1'}}\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "with open(class_file, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Create a dictionary\n",
    "class_dict = {\n",
    "    'classes': json_data['classes'],\n",
    "    'default': json_data['default'],\n",
    "    'type': json_data['type'],\n",
    "    'format': json_data['format'],\n",
    "    'created_with': json_data['created_with']\n",
    "}\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE INITIAL OBJECT PCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=utl.get_list_of_files(input_folder, '.laz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 34_Parking_04_F1_small_pred...\n",
      "Unassigned\n",
      "floors\n",
      "Function fit_planes took 3.8254 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 4.7056 seconds to execute.\n",
      ": 4 clusters found\n",
      "ceilings\n",
      "Function fit_planes took 6.0858 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 7.2780 seconds to execute.\n",
      ": 10 clusters found\n",
      "walls\n",
      "Function fit_planes took 4.6070 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 4.9606 seconds to execute.\n",
      "Function fit_planes took 4.9108 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 5.3295 seconds to execute.\n",
      "Function fit_planes took 5.2451 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 5.6287 seconds to execute.\n",
      "Function fit_planes took 9.1864 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 9.8012 seconds to execute.\n",
      "Function fit_planes took 8.5301 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 9.0943 seconds to execute.\n",
      "Function fit_planes took 4.0163 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 4.2738 seconds to execute.\n",
      "Function fit_planes took 2.9844 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 3.2380 seconds to execute.\n",
      "Function fit_planes took 7.0478 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 7.6509 seconds to execute.\n",
      "Function fit_planes took 1.6450 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 1.8129 seconds to execute.\n",
      "Function fit_planes took 4.7686 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 5.1549 seconds to execute.\n",
      "Function fit_planes took 3.6879 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 3.9554 seconds to execute.\n",
      "Function fit_planes took 4.5510 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 4.8393 seconds to execute.\n",
      ": 100 clusters found\n",
      "columns\n",
      ": 41 clusters found\n",
      "doors\n",
      ": 12 clusters found\n",
      "Created 167 PointCloudNodes created from 34_Parking_04_F1_small_pred\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\test\\34_Parking_04_F1_small_pred.laz\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "for f in files[-1:]:\n",
    "    objectNodes=[]\n",
    "    \n",
    "    # check if las/pcd variable is already defined    \n",
    "    print(f'processing {ut.get_filename(f)}...')      \n",
    "    las = laspy.read(f) #if 'las' not in globals() else las\n",
    "    pcd=gmu.las_to_pcd(las,getNormals=True) #if 'pcd' not in globals() else pcd\n",
    "    \n",
    "    #create thrash node\n",
    "    c=[c for c in class_dict['classes'] if c['id']==255][0]    \n",
    "    thrashNode=t4.create_thrash_node(las,pcd,f,c)\n",
    "    \n",
    "    #CLUSTER OBJECTS\n",
    "    for c in class_dict['classes']:\n",
    "        print(c['name'])\n",
    "        \n",
    "        ###------------------------FLOORS CEILINGS--------------------------------------------\n",
    "        if c['id'] in [0,1]: #floors,ceilings CORRECT\n",
    "            nodes=t4.create_floor_and_ceiling_nodes(las,pcd,f,c,sample_resolution=resolution,\n",
    "                                                                                    distance_threshold=0.05, \n",
    "                                                                                    min_inliers=1000,\n",
    "                                                                                    eps=eps,\n",
    "                                                                                    min_cluster_points=200)\n",
    "            objectNodes.extend(nodes)\n",
    "\n",
    "            print(f': {len(nodes)} clusters found')  \n",
    "        \n",
    "        ###------------------------WALLS--------------------------------------------\n",
    "        if c['id'] in [2]: \n",
    "            nodes,rest_pcd=t4.create_wall_nodes(las,pcd,f,c,sample_resolution=0.03,\n",
    "                                                distance_threshold=0.03, \n",
    "                                                min_inliers=200,\n",
    "                                                eps=eps,\n",
    "                                                threshold_min_cluster_points=threshold_min_cluster_points,\n",
    "                                                size=size,\n",
    "                                                threshold_wall_verticality=threshold_wall_verticality,\n",
    "                                                threshold_wall_dim=threshold_wall_dim,\n",
    "                                                threshold_clustering_distance=threshold_clustering_distance)\n",
    "            objectNodes.extend(nodes)\n",
    "            thrashNode.resource+=rest_pcd\n",
    "            print(f': {len(nodes)} clusters found')\n",
    "     \n",
    "            \n",
    "        # ###------------------------COLUMNS--------------------------------------------\n",
    "        if c['id'] in [3]:\n",
    "            nodes,rest_pcd=t4.create_column_nodes(las,pcd,f,c,eps=1,min_cluster_points=250,\n",
    "                                threshold_column_verticality=threshold_column_verticality,\n",
    "                                threshold_column_height=threshold_column_height,\n",
    "                                threshold_column_points=threshold_column_points)\n",
    "            objectNodes.extend(nodes)\n",
    "            thrashNode.resource+=rest_pcd\n",
    "    \n",
    "            print( f': {len(nodes)} clusters found')       \n",
    "            \n",
    "        # ###------------------------DOORS--------------------------------------------\n",
    "        if c['id'] in [4]: \n",
    "            nodes,rest_pcd=t4.create_door_nodes(las,pcd,f,c,\n",
    "                                            eps=0.5,\n",
    "                                            min_cluster_points=200,\n",
    "                                            threshold_door_dim=0.5)\n",
    "            objectNodes.extend(nodes)\n",
    "            thrashNode.resource+=rest_pcd\n",
    "       \n",
    "            print(f': {len(nodes)} clusters found') \n",
    "    \n",
    "    print(f'Created {len(objectNodes)} PointCloudNodes created from {ut.get_filename(f)}')\n",
    "    \n",
    "    \n",
    "    #EXPORT RESULTS\n",
    "    #merge objects\n",
    "    total_pcd_nodes=objectNodes +[thrashNode]\n",
    "    joined_pcd=gmu.join_geometries([n.resource for n  in total_pcd_nodes])\n",
    "\n",
    "    #export graph\n",
    "    tl.nodes_to_graph(total_pcd_nodes,\n",
    "                    graphPath=str(output_folder/f'{ut.get_filename(f)}.ttl'),\n",
    "                    save=True)\n",
    "    \n",
    "    #obtain labels\n",
    "    labels_segmentation=[]\n",
    "    labels_objects=[]\n",
    "    for i,n in enumerate(total_pcd_nodes):\n",
    "        length=len(np.asarray(n.resource.points))\n",
    "        labels_segmentation.extend(list(np.full(length,n.class_id)))\n",
    "        labels_objects.extend(list(np.full(length,n.object_id)))  \n",
    "    labels_classes=np.array(labels_segmentation)\n",
    "    labels_objects=np.array(labels_objects)\n",
    "    \n",
    "    #create a new las file with the labels\n",
    "    hdr = laspy.LasHeader(version=\"1.4\", point_format=2)\n",
    "    las2 = laspy.LasData(hdr)\n",
    "    las2.x=np.asarray(joined_pcd.points)[:,0]\n",
    "    las2.y=np.asarray(joined_pcd.points)[:,1]\n",
    "    las2.z=np.asarray(joined_pcd.points)[:,2]\n",
    "    las2.red= (np.asarray(joined_pcd.colors)[:,0] * 65535).astype(np.uint16)\n",
    "    las2.green= (np.asarray(joined_pcd.colors)[:,1] * 65535).astype(np.uint16)\n",
    "    las2.blue= (np.asarray(joined_pcd.colors)[:,2] * 65535).astype(np.uint16)\n",
    "\n",
    "    gmu.las_add_extra_dimensions(las2,(labels_classes,labels_objects),['classes','objects'],['uint8','uint16'])\n",
    "    # Write the LAS file as LAZ\n",
    "    las2.write(str(output_folder/f'{ut.get_filename(f)}.laz'))\n",
    "    print(str(output_folder/f'{ut.get_filename(f)}.laz'))\n",
    "    print('DONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {key:value for key, value in objectNodes[1].__dict__.items() if not key.startswith('__') and not callable(key)}              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcd_slice=t2.slice_point_cloud(pcd, -100, pcd.get_center()[2])\n",
    "# joined_pcd=gmu.join_geometries([p.resource.paint_uniform_color(ut.random_color()) for p in objectNodes])\n",
    "# o3d.visualization.draw_geometries([joined_pcd,gmu.sample_geometry(pcd_slice)[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomapi_installed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
