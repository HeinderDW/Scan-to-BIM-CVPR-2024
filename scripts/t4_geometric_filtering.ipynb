{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4 Combine Detection Results\n",
    "\n",
    "- import the t1 inferenced point clouds\n",
    "- import the t3 doors \n",
    "- export graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#IMPORT PACKAGES\n",
    "from rdflib import Graph, URIRef\n",
    "import os.path\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import open3d as o3d\n",
    "import uuid    \n",
    "import pye57 \n",
    "import ifcopenshell\n",
    "import ifcopenshell.geom as geom\n",
    "import ifcopenshell.util\n",
    "from ifcopenshell.util.selector import Selector\n",
    "import multiprocessing\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "# from tabulate import tabulate\n",
    "import cv2\n",
    "import laspy\n",
    "\n",
    "import geomapi\n",
    "from geomapi.nodes import *\n",
    "import geomapi.utils as ut\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import geomapi.tools as tl\n",
    "import geomapi.tools.progresstools as pt\n",
    "\n",
    "#import utils\n",
    "import context \n",
    "import utils as utl\n",
    "import utils.t1_utils as t1\n",
    "import utils.t2_utils as t2\n",
    "import utils.t4_utils as t4\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\n"
     ]
    }
   ],
   "source": [
    "#paths\n",
    "path=Path(os.getcwd()).parents[2]\n",
    "\n",
    "print(path)\n",
    "input_folder=path/'data'/'t2'/'test' \n",
    "class_file=path/'data'/'_classes.json'\n",
    "output_folder=path/'data'/'t4'/ 'test'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#parameters\n",
    "resolution=0.05\n",
    "min_cluster_points=1000\n",
    "eps=0.5\n",
    "\n",
    "#doors\n",
    "threshold_door_dim=1.6#m\n",
    "\n",
    "#walls\n",
    "threshold_clustering_distance=0.4\n",
    "threshold_min_cluster_points=500\n",
    "threshold_wall_verticality=0.2# angle to horizontal\n",
    "threshold_wall_dim=0.5\n",
    "size=[12,12,100] #size wall boxes\n",
    "\n",
    "#columns\n",
    "threshold_column_points=1000\n",
    "threshold_column_height=1.5#m\n",
    "threshold_column_verticality=0.2# angle to horizontal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': [{'name': 'unassigned', 'id': 255, 'temp_id': 0, 'color': '#9da2ab'}, {'name': 'floors', 'id': 0, 'temp_id': 1, 'color': '#03c2fc'}, {'name': 'ceilings', 'id': 1, 'temp_id': 2, 'color': '#e81416'}, {'name': 'walls', 'id': 2, 'temp_id': 3, 'color': '#ffa500'}, {'name': 'columns', 'id': 3, 'temp_id': 4, 'color': '#faeb36'}, {'name': 'doors', 'id': 4, 'temp_id': 5, 'color': '#79c314'}], 'default': 255, 'type': 'semantic_segmentation', 'format': 'kitti', 'created_with': {'name': 'Saiga', 'version': '1.0.1'}}\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "with open(class_file, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Create a dictionary\n",
    "class_dict = {\n",
    "    'classes': json_data['classes'],\n",
    "    'default': json_data['default'],\n",
    "    'type': json_data['type'],\n",
    "    'format': json_data['format'],\n",
    "    'created_with': json_data['created_with']\n",
    "}\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE INITIAL OBJECT PCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=utl.get_list_of_files(input_folder, '.laz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 34_Parking_04_F1_small_pred...\n",
      "Unassigned\n",
      "floors\n",
      "Function fit_planes took 3.8254 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 4.7056 seconds to execute.\n",
      ": 4 clusters found\n",
      "ceilings\n",
      "Function fit_planes took 6.0858 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 7.2780 seconds to execute.\n",
      ": 10 clusters found\n",
      "walls\n",
      "Function fit_planes took 4.6070 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 4.9606 seconds to execute.\n",
      "Function fit_planes took 4.9108 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 5.3295 seconds to execute.\n",
      "Function fit_planes took 5.2451 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 5.6287 seconds to execute.\n",
      "Function fit_planes took 9.1864 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 9.8012 seconds to execute.\n",
      "Function fit_planes took 8.5301 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 9.0943 seconds to execute.\n",
      "Function fit_planes took 4.0163 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 4.2738 seconds to execute.\n",
      "Function fit_planes took 2.9844 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 3.2380 seconds to execute.\n",
      "Function fit_planes took 7.0478 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 7.6509 seconds to execute.\n",
      "Function fit_planes took 1.6450 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 1.8129 seconds to execute.\n",
      "Function fit_planes took 4.7686 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 5.1549 seconds to execute.\n",
      "Function fit_planes took 3.6879 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 3.9554 seconds to execute.\n",
      "Function fit_planes took 4.5510 seconds to execute.\n",
      "Function split_point_cloud_in_planar_clusters took 4.8393 seconds to execute.\n",
      ": 100 clusters found\n",
      "columns\n",
      ": 41 clusters found\n",
      "doors\n",
      ": 12 clusters found\n",
      "Created 167 PointCloudNodes created from 34_Parking_04_F1_small_pred\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\test\\34_Parking_04_F1_small_pred.laz\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "for f in files[-1:]:\n",
    "    objectNodes=[]\n",
    "    \n",
    "    # check if las/pcd variable is already defined    \n",
    "    print(f'processing {ut.get_filename(f)}...')      \n",
    "    las = laspy.read(f) #if 'las' not in globals() else las\n",
    "    pcd=gmu.las_to_pcd(las,getNormals=True) #if 'pcd' not in globals() else pcd\n",
    "    \n",
    "    #create thrash node\n",
    "    c=[c for c in class_dict['classes'] if c['id']==255][0]    \n",
    "    thrashNode=t4.create_thrash_node(las,pcd,f,c)\n",
    "    \n",
    "    #CLUSTER OBJECTS\n",
    "    for c in class_dict['classes']:\n",
    "        print(c['name'])\n",
    "        \n",
    "        ###------------------------FLOORS CEILINGS--------------------------------------------\n",
    "        if c['id'] in [0,1]: #floors,ceilings CORRECT\n",
    "            nodes=t4.create_floor_and_ceiling_nodes(las,pcd,f,c,sample_resolution=resolution,\n",
    "                                                                                    distance_threshold=0.05, \n",
    "                                                                                    min_inliers=1000,\n",
    "                                                                                    eps=eps,\n",
    "                                                                                    min_cluster_points=200)\n",
    "            objectNodes.extend(nodes)\n",
    "\n",
    "            print(f': {len(nodes)} clusters found')  \n",
    "        \n",
    "        ###------------------------WALLS--------------------------------------------\n",
    "        if c['id'] in [2]: \n",
    "            nodes,rest_pcd=t4.create_wall_nodes(las,pcd,f,c,sample_resolution=0.03,\n",
    "                                                distance_threshold=0.03, \n",
    "                                                min_inliers=200,\n",
    "                                                eps=eps,\n",
    "                                                threshold_min_cluster_points=threshold_min_cluster_points,\n",
    "                                                size=size,\n",
    "                                                threshold_wall_verticality=threshold_wall_verticality,\n",
    "                                                threshold_wall_dim=threshold_wall_dim,\n",
    "                                                threshold_clustering_distance=threshold_clustering_distance)\n",
    "            objectNodes.extend(nodes)\n",
    "            thrashNode.resource+=rest_pcd\n",
    "            print(f': {len(nodes)} clusters found')\n",
    "     \n",
    "            \n",
    "        # ###------------------------COLUMNS--------------------------------------------\n",
    "        if c['id'] in [3]:\n",
    "            nodes,rest_pcd=t4.create_column_nodes(las,pcd,f,c,eps=1,min_cluster_points=250,\n",
    "                                threshold_column_verticality=threshold_column_verticality,\n",
    "                                threshold_column_height=threshold_column_height,\n",
    "                                threshold_column_points=threshold_column_points)\n",
    "            objectNodes.extend(nodes)\n",
    "            thrashNode.resource+=rest_pcd\n",
    "    \n",
    "            print( f': {len(nodes)} clusters found')       \n",
    "            \n",
    "        # ###------------------------DOORS--------------------------------------------\n",
    "        if c['id'] in [4]: \n",
    "            nodes,rest_pcd=t4.create_door_nodes(las,pcd,f,c,\n",
    "                                            eps=0.5,\n",
    "                                            min_cluster_points=200,\n",
    "                                            threshold_door_dim=0.5)\n",
    "            objectNodes.extend(nodes)\n",
    "            thrashNode.resource+=rest_pcd\n",
    "       \n",
    "            print(f': {len(nodes)} clusters found') \n",
    "    \n",
    "    print(f'Created {len(objectNodes)} PointCloudNodes created from {ut.get_filename(f)}')\n",
    "    \n",
    "    \n",
    "    #EXPORT RESULTS\n",
    "    #merge objects\n",
    "    total_pcd_nodes=objectNodes +[thrashNode]\n",
    "    joined_pcd=gmu.join_geometries([n.resource for n  in total_pcd_nodes])\n",
    "\n",
    "    #export graph\n",
    "    tl.nodes_to_graph(total_pcd_nodes,\n",
    "                    graphPath=str(output_folder/f'{ut.get_filename(f)}.ttl'),\n",
    "                    save=True)\n",
    "    \n",
    "    #obtain labels\n",
    "    labels_segmentation=[]\n",
    "    labels_objects=[]\n",
    "    for i,n in enumerate(total_pcd_nodes):\n",
    "        length=len(np.asarray(n.resource.points))\n",
    "        labels_segmentation.extend(list(np.full(length,n.class_id)))\n",
    "        labels_objects.extend(list(np.full(length,n.object_id)))  \n",
    "    labels_classes=np.array(labels_segmentation)\n",
    "    labels_objects=np.array(labels_objects)\n",
    "    \n",
    "    #create a new las file with the labels\n",
    "    hdr = laspy.LasHeader(version=\"1.4\", point_format=2)\n",
    "    las2 = laspy.LasData(hdr)\n",
    "    las2.x=np.asarray(joined_pcd.points)[:,0]\n",
    "    las2.y=np.asarray(joined_pcd.points)[:,1]\n",
    "    las2.z=np.asarray(joined_pcd.points)[:,2]\n",
    "    las2.red= (np.asarray(joined_pcd.colors)[:,0] * 65535).astype(np.uint16)\n",
    "    las2.green= (np.asarray(joined_pcd.colors)[:,1] * 65535).astype(np.uint16)\n",
    "    las2.blue= (np.asarray(joined_pcd.colors)[:,2] * 65535).astype(np.uint16)\n",
    "\n",
    "    gmu.las_add_extra_dimensions(las2,(labels_classes,labels_objects),['classes','objects'],['uint8','uint16'])\n",
    "    # Write the LAS file as LAZ\n",
    "    las2.write(str(output_folder/f'{ut.get_filename(f)}.laz'))\n",
    "    print(str(output_folder/f'{ut.get_filename(f)}.laz'))\n",
    "    print('DONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {key:value for key, value in objectNodes[1].__dict__.items() if not key.startswith('__') and not callable(key)}              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcd_slice=t2.slice_point_cloud(pcd, -100, pcd.get_center()[2])\n",
    "# joined_pcd=gmu.join_geometries([p.resource.paint_uniform_color(ut.random_color()) for p in objectNodes])\n",
    "# o3d.visualization.draw_geometries([joined_pcd,gmu.sample_geometry(pcd_slice)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE GRAPHS TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\n"
     ]
    }
   ],
   "source": [
    "#paths\n",
    "path=Path(os.getcwd()).parents[2]\n",
    "\n",
    "print(path)\n",
    "input_folder=path/'data'/'t1'/'train' \n",
    "class_file=path/'data'/'_classes.json'\n",
    "output_folder=path/'data'/'t4'/ 'train'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05_MedOffice_01_F2_small1_doors_4000'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut.get_filename(f)+'_'+c['name']+'_'+str(c['id']*1000+counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 05_MedOffice_01_F2_small1 ...\n",
      "289 Nodes found\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\train\\05_MedOffice_01_F2_small1.laz\n",
      "DONE\n",
      "processing 19_MedOffice_07_F4_small1 ...\n",
      "206 Nodes found\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\train\\19_MedOffice_07_F4_small1.laz\n",
      "DONE\n",
      "processing 32_ShortOffice_05_F1_small1 ...\n",
      "174 Nodes found\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\train\\32_ShortOffice_05_F1_small1.laz\n",
      "DONE\n",
      "processing 32_ShortOffice_05_F2_small1 ...\n",
      "219 Nodes found\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\train\\32_ShortOffice_05_F2_small1.laz\n",
      "DONE\n",
      "processing 32_ShortOffice_05_F3_small1 ...\n",
      "214 Nodes found\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\train\\32_ShortOffice_05_F3_small1.laz\n",
      "DONE\n",
      "processing 33_SmallBuilding_03_F1_small1 ...\n",
      "277 Nodes found\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\train\\33_SmallBuilding_03_F1_small1.laz\n",
      "DONE\n",
      "processing 35_Lab_02_F1_small1 ...\n",
      "432 Nodes found\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\train\\35_Lab_02_F1_small1.laz\n",
      "DONE\n",
      "processing 35_Lab_02_F2_small1 ...\n",
      "127 Nodes found\n",
      "c:\\Users\\u0094523\\OneDrive - KU Leuven\\2024-05 CVPR scan-to-BIM challenge\\data\\t4\\train\\35_Lab_02_F2_small1.laz\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "files=utl.get_list_of_files(input_folder,'.laz')\n",
    "\n",
    "for f in files: \n",
    "    pcdNodes=[]\n",
    "    #import pcd and check if las/pcd variable is already defined    \n",
    "    print(f'processing {ut.get_filename(f)} ...')      \n",
    "    las = laspy.read(f) #○if 'las' not in globals() else las\n",
    "    pcd=gmu.las_to_pcd(las) #if 'pcd' not in globals() else pcd # this is the slowest step\n",
    "        \n",
    "    #match pcd to nodes\n",
    "    for c in class_dict['classes']:\n",
    "        idx=np.where((las['classes']==c['id']))[0]\n",
    "        class_pcd=pcd.select_by_index(idx)\n",
    "        object_labels=las['objects'][idx]\n",
    "        counter=0\n",
    "        for j in np.unique(object_labels):\n",
    "            indices=np.where(object_labels==j)[0]\n",
    "            object_pcd=class_pcd.select_by_index(indices)\n",
    "            pcdNodes.append(PointCloudNode(resource=object_pcd,\n",
    "                                                class_id=c['id'],\n",
    "                                                class_name=c['name'],\n",
    "                                                object_id=c['id']*1000+counter, \n",
    "                                                color=ut.random_color(),\n",
    "                                                name=ut.get_filename(f)+'_'+c['name']+'_'+str(c['id']*1000+counter))) \n",
    "            counter+=1\n",
    "    print(f'{len(pcdNodes)} Nodes found')     \n",
    "        \n",
    "\n",
    "   #export graph\n",
    "    tl.nodes_to_graph(pcdNodes,\n",
    "                    graphPath=str(output_folder/f'{ut.get_filename(f)}.ttl'),\n",
    "                    save=True)\n",
    "                                                \n",
    "    \n",
    "    #obtain labels\n",
    "    joined_pcd=gmu.join_geometries([n.resource for n  in pcdNodes])\n",
    "    labels_segmentation=[]\n",
    "    labels_objects=[]\n",
    "    for i,n in enumerate(pcdNodes):\n",
    "        length=len(np.asarray(n.resource.points))\n",
    "        labels_segmentation.extend(list(np.full(length,n.class_id)))\n",
    "        labels_objects.extend(list(np.full(length,n.object_id)))  \n",
    "    labels_classes=np.array(labels_segmentation)\n",
    "    labels_objects=np.array(labels_objects)\n",
    "    \n",
    "    #create a new las file with the labels\n",
    "    hdr = laspy.LasHeader(version=\"1.4\", point_format=2)\n",
    "    las2 = laspy.LasData(hdr)\n",
    "    las2.x=np.asarray(joined_pcd.points)[:,0]\n",
    "    las2.y=np.asarray(joined_pcd.points)[:,1]\n",
    "    las2.z=np.asarray(joined_pcd.points)[:,2]\n",
    "    las2.red= (np.asarray(joined_pcd.colors)[:,0] * 65535).astype(np.uint16)\n",
    "    las2.green= (np.asarray(joined_pcd.colors)[:,1] * 65535).astype(np.uint16)\n",
    "    las2.blue= (np.asarray(joined_pcd.colors)[:,2] * 65535).astype(np.uint16)\n",
    "\n",
    "    gmu.las_add_extra_dimensions(las2,(labels_classes,labels_objects),['classes','objects'],['uint8','uint32'])\n",
    "    # Write the LAS file as LAZ\n",
    "    las2.write(str(output_folder/f'{ut.get_filename(f)}.laz'))\n",
    "    print(str(output_folder/f'{ut.get_filename(f)}.laz'))\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_e57Index': 0,\n",
       " 'pointCount': 29904,\n",
       " 'e57XmlPath': None,\n",
       " '_cartesianBounds': array([-1.01535794e+00, -6.63357943e-01,  5.37251209e+00,  9.95851209e+00,\n",
       "        -1.96010014e-03,  2.75003990e+00]),\n",
       " '_orientedBounds': array([[-1.01159518,  9.93890469,  2.94928465],\n",
       "        [-1.05432483,  5.25668146,  2.74052925],\n",
       "        [-1.01632944, 10.06852291,  0.04301649],\n",
       "        [-0.6311647 ,  9.93546737,  2.94851163],\n",
       "        [-0.6786286 ,  5.38286236, -0.16651193],\n",
       "        [-0.63589895, 10.06508559,  0.04224347],\n",
       "        [-0.67389435,  5.25324414,  2.73975623],\n",
       "        [-1.05905908,  5.38629968, -0.16573891]]),\n",
       " '_orientedBoundingBox': OrientedBoundingBox: center: (-0.845112, 7.66088, 1.39139), extent: 4.68707, 2.90916, 0.380447),\n",
       " '_subject': rdflib.term.URIRef('file:///05_MedOffice_01_F2_small1_walls_47648'),\n",
       " '_graph': <Graph identifier=N4baefcd50a7a424e847ed8bb27073601 (<class 'rdflib.graph.Graph'>)>,\n",
       " '_graphPath': 'c:\\\\Users\\\\u0094523\\\\OneDrive - KU Leuven\\\\2024-05 CVPR scan-to-BIM challenge\\\\data\\\\t4\\\\train\\\\05_MedOffice_01_F2_small1.ttl',\n",
       " '_path': None,\n",
       " '_name': '05_MedOffice_01_F2_small1_walls_47648',\n",
       " '_timestamp': None,\n",
       " '_resource': PointCloud with 29904 points.,\n",
       " '_cartesianTransform': array([[ 1.        ,  0.        ,  0.        , -0.8804589 ],\n",
       "        [ 0.        ,  1.        ,  0.        ,  7.65001222],\n",
       "        [ 0.        ,  0.        ,  1.        ,  1.83904094],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " 'class_id': 2,\n",
       " 'class_name': 'walls',\n",
       " 'object_id': 47648,\n",
       " 'color': array([0.45490196, 0.66666667, 0.01960784])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcdNodes[100].__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomapi_installed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
